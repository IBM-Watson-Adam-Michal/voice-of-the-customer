{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Flow\n",
    "\n",
    "The processing flow is run on all of the review data once the models are all trained up and validated. This is the flow that will actually turn the raw data into processed output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Token Replacement\n",
    "\n",
    "This step allows Alchemy to replace words in the sentences of the reviews by their semantic types (product, customer_service, company, etc). These semantic types were defined when the WKS model was trained and are usually associated with a given domain data.\n",
    "\n",
    "It replaces tokens from the 'reviewText' field of the documents stored in the database. It saves the replaced sentences to the 'taggedRevie' field in the same document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import csv\n",
    "from watson_developer_cloud import alchemy_language_v1 as alchemy\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "logger.debug(config.sections())\n",
    "\n",
    "model_id = config['WKS']['WKS_MODEL_ID']\n",
    "alchemy_api = alchemy.AlchemyLanguageV1(api_key = \n",
    "                    config['ALCHEMY']['ALCHEMY_API_KEY'])\n",
    "\n",
    "\n",
    "def get_entities(review):\n",
    "    split = {}\n",
    "    if len(review) > 5024:\n",
    "        mid = find_middle(review)\n",
    "        while mid >= 5024:\n",
    "            mid = find_middle(review[:mid])\n",
    "        review = review[:mid]\n",
    "        half = review[mid:]\n",
    "        split = get_entities(half)\n",
    "    f = alchemy_api.entities(text=review, model='', sentiment=True)\n",
    "    response = f.content\n",
    "    response = ast.literal_eval(response)\n",
    "    if split != {}:\n",
    "        if 'entities' in split and 'entities' in response:\n",
    "            response['entities'] = response['entities'] + split['entities']\n",
    "            response['text'] = response['text'] + split['text']\n",
    "        elif 'entites' in split and 'entities' not in response:\n",
    "            response['entities'] = split['entities']\n",
    "    return response\n",
    "\n",
    "\n",
    "def token_replacement_entities(review):\n",
    "    processed = get_entities(review)\n",
    "    if 'statusInfo' in processed:\n",
    "        return review\n",
    "    if 'entities' in processed:\n",
    "        entities = processed['entities']\n",
    "        text = processed['text']\n",
    "        for i in entities:\n",
    "            token = i['text']\n",
    "            classification = \"<\" + i['type'] + \">\"\n",
    "            token = re.escape(token)\n",
    "            re.sub(r'\\\\ ', ' ', token)\n",
    "            text = re.sub(r\"\\b%s\\b\" % token, classification, text, count=1)\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_middle(text):\n",
    "        generator = nltk.tokenize.util.regexp_span_tokenize(text, r'\\.')\n",
    "        sequences = list(generator)\n",
    "        mid_sentence = len(sequences)/2\n",
    "        middle_char = sequences[mid_sentence][1]\n",
    "        middle_char = int(middle_char) + 1\n",
    "        return middle_char\n",
    "    \n",
    "#Initializing Cloudant client\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "\n",
    "#Going through all the documents and replacing the tokens by\n",
    "#their semantic types. Result is save back to the Cloudant document\n",
    "#in the 'taggedReview' field.\n",
    "client.connect()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "for doc in db:\n",
    "    logger.debug(doc)\n",
    "    try:\n",
    "        doc['taggedReview'] = token_replacement_entities(doc['reviewText'])\n",
    "        doc.save()\n",
    "    except:\n",
    "        logger.error('Error saving tagged review to Cloudant document.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification of reviews\n",
    "\n",
    "This step uses the Natural Language Classifier (NLC) created on the Training notebook. This step classifies a review and adds the result of the classification to the 'class' field of the document in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import cloudant\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "\n",
    "NLC_USERNAME = config['NLC']['NLC_USERNAME']\n",
    "NLC_PASSWORD = config['NLC']['NLC_PASSWORD']\n",
    "NLC_CLASSIFIER = config['NLC']['NLC_CLASSIFIER']\n",
    "\n",
    "#initializing classifier object\n",
    "nlc = NaturalLanguageClassifierV1(username=NLC_USERNAME, \n",
    "                                  password=NLC_PASSWORD)\n",
    "\n",
    "def classify(review):\n",
    "    logger.debug(review)\n",
    "    #Classify sentence\n",
    "    try:\n",
    "        response = nlc.classify(NLC_CLASSIFIER, review)\n",
    "        logger.debug(response)\n",
    "        if len(response['classes']) > 1:\n",
    "            return response['classes']\n",
    "    except:\n",
    "        logger.error('Failed at sentence classification')\n",
    "        return 'no class'\n",
    "\n",
    "#Initializing Cloudant client\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "\n",
    "client.connect()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "for doc in db:\n",
    "    logger.debug(doc)\n",
    "    try:\n",
    "        doc['class'] = classify(doc['taggedReview'])\n",
    "        doc.save()\n",
    "    except:\n",
    "        logger.error('Error saving classification to Cloudant document.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grouping products that have reviews\n",
    "\n",
    "This script will create a product document in the database for each product that has been reviewed. It will also attach a list of the review id's to the new product document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Samsung UN19F4000 19-Inch 720p 60Hz Slim LED HDTV': {'reviewCount': 164, 'product_id': u'B00BCGRZ04', 'customer_service': {'sentiment': {}}, 'product_name': u'Samsung UN19F4000 19-Inch 720p 60Hz Slim LED HDTV', 'issues': {}, 'features': []}, u'Bose QuietComfort 15 Acoustic Noise Cancelling Headphones': {'reviewCount': 804, 'product_id': u'B0054JJ0QW', 'customer_service': {'sentiment': {}}, 'product_name': u'Bose QuietComfort 15 Acoustic Noise Cancelling Headphones', 'issues': {}, 'features': []}, u'Microsoft Comfort Mouse 4500': {'reviewCount': 118, 'product_id': u'B003BEDPHM', 'customer_service': {'sentiment': {}}, 'product_name': u'Microsoft Comfort Mouse 4500', 'issues': {}, 'features': []}}\n",
      "Samsung UN19F4000 19-Inch 720p 60Hz Slim LED HDTV\n",
      "164\n",
      "Bose QuietComfort 15 Acoustic Noise Cancelling Headphones\n",
      "804\n",
      "Microsoft Comfort Mouse 4500\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "from cloudant.client import Cloudant\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "client.connect()\n",
    "db = client[DATABASE]\n",
    "products = {}\n",
    "for doc in db:\n",
    "    if doc['title']:\n",
    "        if doc['title'] not in products:\n",
    "            products[doc['title']] = {}\n",
    "            products[doc['title']]['product_name'] = doc['title']\n",
    "            products[doc['title']]['customer_service'] = {'sentiment': {}}\n",
    "            products[doc['title']]['customer_service']['sentiment']['posCount'] = 0\n",
    "            products[doc['title']]['customer_service']['sentiment']['neuCount'] = 0\n",
    "            products[doc['title']]['customer_service']['sentiment']['negCount'] = 0\n",
    "            products[doc['title']]['features'] = []\n",
    "            products[doc['title']]['issues'] = {'count': 0}\n",
    "            products[doc['title']]['reviewCount'] = 0\n",
    "            products[doc['title']]['product_id'] = doc['asin']\n",
    "    \n",
    "            products[doc['title']]['reviewCount'] += 1\n",
    "        if doc['title']['sentiment'] == 'positive':\n",
    "            products[doc['title']]['customer_service']['sentiment']['posCount'] += 1\n",
    "        elif doc['title']['sentiment'] == 'neutral':\n",
    "            products[doc['title']]['customer_service']['sentiment']['neuCount'] += 1\n",
    "        elif doc['title']['sentiment'] == 'negative':\n",
    "            products[doc['title']]['customer_service']['sentiment']['negCount'] += 1\n",
    "    \n",
    "    #Add detected features list. I'm not sure how it's being stored during the alchemy call atm\n",
    "    #products[doc['title']]['features'].append(doc['title']['features'])\n",
    "    \n",
    "    #There is probably some other stuff that needs to be formatted here\n",
    "    #Until we can step through the whole process this is just kinda a guess\n",
    "    \n",
    "for product, value in products.items():\n",
    "    print product\n",
    "    #Need to push product to db whenever we're done here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering product feature sentences\n",
    "\n",
    "The script that runs the clustering on the review data is src/Processing/clustering.py\n",
    "A word2vec model is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cloudant.client import Cloudant\n",
    "from cloudant.query import Query\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "def generate_vectors(features, model):\n",
    "    vecs = []\n",
    "    mapping = []\n",
    "    count = 0\n",
    "    for line in features:\n",
    "        words = line['word'].split()\n",
    "        vec = []\n",
    "        flag = 0\n",
    "        for word in words:\n",
    "            word = str(word)\n",
    "            word = re.escape(word)\n",
    "            word = re.sub(r'\\\\', '', word)\n",
    "            if word in model:\n",
    "                if len(vec) > 1:\n",
    "                    vec = vec+model[word]\n",
    "                else:\n",
    "                    vec = model[word]\n",
    "            else:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            if len(vec) > 0:\n",
    "                vecs.append(vec)\n",
    "                mapping.append(count)\n",
    "        count += 1\n",
    "    return [vecs, mapping]\n",
    "\n",
    "\n",
    "def cluster_try(vecs):\n",
    "        clusterVec = {}\n",
    "        clusterIdx = {}\n",
    "        no_of_clusters = 1\n",
    "        clusterIdx[0] = [0]\n",
    "        clusterVec[0] = vecs[0]\n",
    "        max_sim = 0.5\n",
    "        index = 0\n",
    "        for i in range(1, len(vecs)):\n",
    "                flag = 0\n",
    "                max_sim = 0.5\n",
    "                for j in range(no_of_clusters):\n",
    "                        sim = np.dot(vecs[i], clusterVec[j])/(np.linalg.norm(clusterVec[j]) * np.linalg.norm(vecs[i]))\n",
    "                        if sim > max_sim:\n",
    "                                flag = 1\n",
    "                                max_sim = sim\n",
    "                                index = j\n",
    "                if flag == 0:\n",
    "                        clusterIdx[j+1] = [i]\n",
    "                        clusterVec[j+1] = vecs[i]\n",
    "                        no_of_clusters += 1\n",
    "                else:\n",
    "                        clusterIdx[index].append(i)\n",
    "                        clusterVec[index] += vecs[i]\n",
    "        return clusterIdx\n",
    "\n",
    "##I think this function can be deleted or at least drastically reduced given the change to groupings done earlier\n",
    "def create_json(clusters, cluster_data, mapping, keys, helpful, local_dump):\n",
    "    for i in clusters:\n",
    "        keyword_count = 0\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        neutral = 0\n",
    "        unique_words = {}\n",
    "        clusterinfo = {}\n",
    "        for key in clusters[i]:\n",
    "            index = mapping[key]\n",
    "            keyword = keys[index]['word']\n",
    "            if keyword in unique_words:\n",
    "                unique_words[keyword]['count'] += 1\n",
    "                unique_words[keyword]['review_id'].append(keys[index]['rev_id'])\n",
    "                unique_words[keyword]['sentence_id'].append(keys[index]['sentence_id'])\n",
    "            else:\n",
    "                unique_words[keyword] = {}\n",
    "                unique_words[keyword]['count'] = 1\n",
    "                unique_words[keyword]['review_id'] = [keys[index]['rev_id']]\n",
    "                unique_words[keyword]['sentence_id'] = [keys[index]['sentence_id']]\n",
    "            keyword_count += 1\n",
    "            list_keywords = []\n",
    "            stop_count = 0\n",
    "            for feature in sorted(unique_words, key=unique_words.get, reverse=True):\n",
    "                data = {}\n",
    "                data['keyword'] = feature\n",
    "                if stop_count == 0:\n",
    "                    clusterinfo['feature'] = feature\n",
    "                data['sentence_id'] = unique_words[feature]['sentence_id']\n",
    "                data['review_id'] = unique_words[feature]['review_id']\n",
    "                helpful_vote=0\n",
    "                for index_rev in range(0,len(data['review_id'])):\n",
    "                    if helpful[data['review_id'][index_rev]]>=helpful_vote:\n",
    "                        helpful_vote=helpful[data['review_id'][index_rev]]\n",
    "                        helpful_rev=index_rev\n",
    "\n",
    "                sent_id=data['sentence_id'][helpful_rev]\n",
    "                helpful_review=local_dump[data['review_id'][helpful_rev]]\n",
    "\n",
    "                ##cause of split reviews-to remove\n",
    "                sent_id=sent_id-helpful_review[0][0]['seqno']\n",
    "                if sent_id>0:\n",
    "                    excerpt=helpful_review[0][sent_id-1]['sentence']+helpful_review[0][sent_id]['sentence']\n",
    "                else:\n",
    "                    excerpt=helpful_review[0][sent_id]['sentence']\n",
    "                if sent_id<len(helpful_review[0])-1:\n",
    "                    excerpt=excerpt+helpful_review[0][sent_id+1]['sentence']\n",
    "\n",
    "                data['excerpt']=excerpt\n",
    "\n",
    "                data['count'] = unique_words[feature]['count']\n",
    "                list_keywords.append(data)\n",
    "                stop_count += 1\n",
    "                if stop_count == 3:\n",
    "                    break\n",
    "            if keys[index]['sentiment'][0][0] == 'positive':\n",
    "                    pos += 1\n",
    "            if keys[index]['sentiment'][0][0] == 'neutral':\n",
    "                    neutral += 1\n",
    "            if keys[index]['sentiment'][0][0] == 'negative':\n",
    "                    neg += 1\n",
    "        clusterinfo['keywords'] = list_keywords\n",
    "        clusterinfo['sentiments'] = {}\n",
    "        clusterinfo['sentiments']['positive'] = pos\n",
    "        clusterinfo['sentiments']['negative'] = neg\n",
    "        clusterinfo['sentiments']['neutral'] = neutral\n",
    "        clusterinfo['keyword_count'] = keyword_count\n",
    "        cluster_data.append(clusterinfo)\n",
    "    return cluster_data\n",
    "\n",
    "   \n",
    "    #Since we did grouping in the previous step this shouldn't be necessary\n",
    "    ####start###############\n",
    "#def cluster(doc, db, asin):\n",
    "    #SERVER = 'https://1790ef54-fcf2-4029-9b73-9000dff88e6e-bluemix.cloudant.com'\n",
    "    #DATABASE = 'amazon_data'\n",
    "    #USERNAME = '1790ef54-fcf2-4029-9b73-9000dff88e6e-bluemix'\n",
    "    #PASSWORD = '5beb3f8b9f95586542e3d9c5acfb0c52832252432623e534d4e88b12fad29638'\n",
    "\n",
    "    #server = cloudant.client.Cloudant(USERNAME, PASSWORD, url=SERVER)\n",
    "    #server.connect()\n",
    "    #db = server[DATABASE]\n",
    "    #query = Query(db, selector={'asin': asin, 'type':['review']},fields=[\"_id\", \"helpful\"])\n",
    "    #meta = Query(db, selector={'asin': asin, 'type': ['metadata']})\n",
    "    #meta = meta.result[0][0]\n",
    "    #name = ''\n",
    "    #if 'title' in meta:\n",
    "    #    name = meta['title']\n",
    "    #rev_id = []\n",
    "    #helpful={}\n",
    "    #for data in query.result:\n",
    "    #    rev_id.append(data['_id'])\n",
    "    #    if 'helpful' in data:\n",
    "    #        helpful[data['_id']]=data['helpful'][0]\n",
    "    #    else:\n",
    "    #        helpful[data['_id']]=0\n",
    "\n",
    "    #temp = {}\n",
    "    #keys = []\n",
    "    #local_dump = {}\n",
    "    #for rev in rev_id:\n",
    "    #   query_id = Query(db, selector={'review_id': rev, 'type': ['classified']})\n",
    "    #   for i in query_id.result:\n",
    "    #        if len(query_id.result[0]) == 0:\n",
    "    #            continue\n",
    "    #    for res in query_id.result[0]:\n",
    "    #        text = res['review']\n",
    "    #        local_dump[res['review_id']] = text\n",
    "    #        for obj in text[0]:\n",
    "    #            if 'Feature' in obj:\n",
    "    #                feature = obj['Feature']\n",
    "    #                for data in feature:\n",
    "    #                    if 'name' in data:\n",
    "    #                        temp = {}\n",
    "    #                        temp['word'] = data['name']\n",
    "    #                        if 'sentiment' in data:\n",
    "    #                            temp['sentiment'] = data['sentiment']\n",
    "    #                        else:\n",
    "    #                            temp['sentiment'] = ['neutral']\n",
    "    #                        temp['rev_id'] = res['review_id']\n",
    "    #                        temp['sentence_id'] = obj['seqno']\n",
    "    #                        keys.append(temp)\n",
    "    #modelname = 'sample_model'\n",
    "    #cwd = os.getcwd()\n",
    "    #model = word2vec.Word2Vec.load_word2vec_format(cwd+ '/' + modelname+'.bin', binary=True)\n",
    "    #[vecs, mapping] = generate_vectors(keys, model)\n",
    "    #clusters = cluster_try(vecs)\n",
    "    #cluster_data = []\n",
    "    #features = create_json(clusters, cluster_data, mapping, keys, helpful, local_dump)\n",
    "    #features = sorted(features, key=lambda k: k['keyword_count'], reverse=True)\n",
    "\n",
    "    #featureDict = {}\n",
    "    #featureDict['features'] = features[:10]\n",
    "    #featureDict['product_name'] = name\n",
    "    #return featureDict\n",
    "    ###end#############\n",
    "    \n",
    "    \n",
    "#I think it should look something like this instead\n",
    "###start######\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "logger.debug(config.sections())\n",
    "\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "\n",
    "client.connect()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "    \n",
    "#Steps\n",
    "#Find all products\n",
    "#send feature lists to w2v model for clustering\n",
    "#save the clustered feature list back to db\n",
    "modelname = 'sample_model'\n",
    "cwd = os.getcwd()\n",
    "model = word2vec.Word2Vec.load_word2vec_format(cwd+ '/' + modelname+'.bin', binary=True)\n",
    "for doc in db:\n",
    "    if doc['product_name']:\n",
    "        [vecs, mapping] = generate_vectors(doc['features'], model)\n",
    "        clusters = cluster_try(vecs)\n",
    "        #update db with the clusters\n",
    "###end#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Make Final JSON\n",
    "The script that takes all of these pieces and turns them into the final JSON structure that will be used by a front end application is src/Processing/makeFinalJSON.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We might be able to skip this whole step. I did the grouping in an earlier section to make the clustering step simpler.\n",
    "#If we get that formatted correctly there then I think this can be removed. -Andrew\n",
    "\n",
    "from watson_developer_cloud import AlchemyLanguageV1\n",
    "from cloudant.query import Query\n",
    "from cloudant.client import Cloudant\n",
    "\n",
    "\n",
    "JSON_FILE = \"./Model_Clustering.JSON\"\n",
    "outputJSON = {\n",
    "    \"product_name\": \"\",\n",
    "    \"product_id\": None,\n",
    "    \"features\": [],\n",
    "    \"issues\": {\n",
    "        \"percentage\": 0,\n",
    "        \"review_ids\": []\n",
    "    },\n",
    "    \"customer_service\": {\n",
    "        \"sentiment\": {\n",
    "            \"positive\": 0,\n",
    "            \"neutral\": 0,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "DB_USERNAME = ''\n",
    "DB_PASSWORD = ''\n",
    "DB_ACCOUNT = ''\n",
    "DATABASE = ''                                               \n",
    "AL_KEY = ''\n",
    "\n",
    "client = Cloudant(DB_USERNAME, DB_PASSWORD, account=DB_ACCOUNT)\n",
    "client.connect()\n",
    "db = client[DATABASE]\n",
    "\n",
    "def make_final(cluster, db):\n",
    "    outputJSON = {\"product_name\": \"\",\"product_id\": None,\"features\": [],\"issues\": {\"percentage\": 0,\"review_ids\": []},\"customer_service\": {\"sentiment\": {\"positive\": 0,\"neutral\": 0,\"negative\": 0}}}\n",
    "    alchemy = AlchemyLanguageV1(api_key=AL_KEY)\n",
    "\n",
    "    reviewnums = set()\n",
    "    reviews = []\n",
    "\n",
    "    for group in cluster['features']:\n",
    "        for keywords in group['keywords']:\n",
    "            for review_ids in keywords['review_id']:\n",
    "                reviewnums.add(review_ids)\n",
    "    for i in range(0, len(reviewnums)):\n",
    "        if len(reviewnums) > 0:\n",
    "            num = reviewnums.pop()\n",
    "            q = Query(db, selector={'review_id': num})\n",
    "            for i in q.result():\n",
    "                if i['type'] == ['classified']:\n",
    "                    reviews.append(q.result[0])\n",
    "    reviewnums.clear()\n",
    "    total = 0\n",
    "    for review in reviews:\n",
    "        if review != []:\n",
    "            for line in review[0][\"review\"]:\n",
    "                total = total + 1\n",
    "                if type(line) == list:\n",
    "                    line = line[0]\n",
    "                if type(line) == int:\n",
    "                    continue\n",
    "                if(line[\"layer3type\"] == \"Issue\"):\n",
    "                    outputJSON[\"issues\"][\"percentage\"] = outputJSON[\"issues\"][\"percentage\"] + 1\n",
    "                    outputJSON[\"issues\"][\"review_ids\"].append(review[0][\"review_id\"])\n",
    "                if(line[\"layer2type\"] == \"Customer Service\"):\n",
    "                    sentiment = alchemy.sentiment(text=line[\"sentence\"])[\"docSentiment\"][\"type\"]\n",
    "                    outputJSON[\"customer_service\"][\"sentiment\"][sentiment] = outputJSON[\"customer_service\"][\"sentiment\"][sentiment] + 1\n",
    "\n",
    "    outputJSON[\"issues\"][\"percentage\"] = outputJSON[\"issues\"][\"percentage\"]/float(total)*100\n",
    "    customer_service_total = 0\n",
    "    for sentiment in outputJSON[\"customer_service\"][\"sentiment\"]:\n",
    "        customer_service_total = customer_service_total + outputJSON[\"customer_service\"][\"sentiment\"][sentiment]\n",
    "\n",
    "    if(customer_service_total > 0):\n",
    "        for sentiment in outputJSON[\"customer_service\"][\"sentiment\"]:\n",
    "            outputJSON[\"customer_service\"][\"sentiment\"][sentiment] = outputJSON[\"customer_service\"][\"sentiment\"][sentiment]/float(customer_service_total)*100\n",
    "\n",
    "    featureArray = cluster[\"features\"]\n",
    "    total = 0\n",
    "    for item in featureArray:\n",
    "        feature = {}\n",
    "        feature[\"group_name\"] = item[\"feature\"]\n",
    "        feature[\"percentage\"] = item[\"keyword_count\"]\n",
    "        total = total + item[\"keyword_count\"]\n",
    "        feature[\"sentiments\"] = item[\"sentiments\"]\n",
    "        feature[\"keywords\"] = []\n",
    "        for i in range(len(item[\"keywords\"])):\n",
    "            elem = item[\"keywords\"][i]\n",
    "            keyword = {\n",
    "                \"name\": elem[\"keyword\"],\n",
    "                \"review_id\": elem[\"review_id\"],\n",
    "                \"sentence_id\": elem[\"sentence_id\"]\n",
    "            }\n",
    "            feature[\"keywords\"].append(keyword)\n",
    "        for sent in feature[\"sentiments\"]:\n",
    "            feature[\"sentiments\"][sent] = feature[\"sentiments\"][sent]/float(item[\"keyword_count\"])*100\n",
    "        outputJSON[\"features\"].append(feature)\n",
    "\n",
    "    for item in outputJSON[\"features\"]:\n",
    "        item[\"percentage\"] = item[\"percentage\"]/float(total)*100\n",
    "\n",
    "    return outputJSON\n",
    "\n",
    "client = cloudant.client.Cloudant(DB_USERNAME, DB_PASSWORD, account=DB_ACCOUNT)\n",
    "client.connect()\n",
    "db = client[DATABASE]\n",
    "for doc in db:\n",
    "  doc['class'] = classify(doc['reviewText'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
