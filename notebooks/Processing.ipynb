{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Flow\n",
    "The processing flow is run on all of the review data once the models are all trained up and validated. This is the flow that will actually turn the raw data into the processed output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "The entire processing flow is automated in src/Processing/controller.py which runs all the steps for you. However, each of these steps may need to be altered slightly depending on how the data is being stored and the format of the data itself.\n",
    "\n",
    "1. Token Replacement\n",
    "2. Classification\n",
    "3. Clustering\n",
    "4. Make Final JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Replacement\n",
    "The script that runs the token replacement on the review data is src/Processing/token_replacement.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Needs token_replacement.py to be rewritten. Until the alchemy model is working again we can't test this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "The script that runs the NLC on the review data is src/Procesing/runNLC.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DB account data needs to be moved to .env file\n",
    "\n",
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "import sys, os\n",
    "#probably can remove this \n",
    "from utils import classify as clf\n",
    "\n",
    "def classify(review):\n",
    "    for line in review['review'][0]:\n",
    "        print(line)\n",
    "        sentence = \"\"\n",
    "        if('replaced_sentence' in line):\n",
    "            sentence = line['replaced_sentence']\n",
    "        else:\n",
    "            sentence = line['sentence']\n",
    "            #Classifier layer stuff that needs to be removed\n",
    "        if len(sentence) < 1024:\n",
    "            line['layer1type'] = clf.classify(sentence)\n",
    "        else:\n",
    "            line['layer1type'] = 'Sentence too long to Classify'\n",
    "    review['type'] = ['classified']\n",
    "    return review\n",
    "\n",
    "DB_USERNAME = '204f49bc-b226-413d-8dcf-aece9c16ce89-bluemix'\n",
    "DB_PASSWORD = 'b0527958a389317a174f7042c2b5edc94a786a0cf3e62fb83ae9dcb51b62dadd'\n",
    "DB_ACCOUNT = '204f49bc-b226-413d-8dcf-aece9c16ce89-bluemix'\n",
    "DATABASE = 'dbtest'\n",
    "\n",
    "client = cloudant.client.Cloudant(DB_USERNAME, DB_PASSWORD, account=DB_ACCOUNT)\n",
    "client.connect()\n",
    "db = client[DATABASE]\n",
    "for doc in db:\n",
    "  doc['class'] = classify(doc['reviewText'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUPING\n",
    "\n",
    "This script will create a product document in the database for each product that has been reviewed. It will also attach a list of the review id's to the new product document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Samsung UN19F4000 19-Inch 720p 60Hz Slim LED HDTV': {'reviewCount': 164, 'product_id': u'B00BCGRZ04', 'customer_service': {'sentiment': {}}, 'product_name': u'Samsung UN19F4000 19-Inch 720p 60Hz Slim LED HDTV', 'issues': {}, 'features': []}, u'Bose QuietComfort 15 Acoustic Noise Cancelling Headphones': {'reviewCount': 804, 'product_id': u'B0054JJ0QW', 'customer_service': {'sentiment': {}}, 'product_name': u'Bose QuietComfort 15 Acoustic Noise Cancelling Headphones', 'issues': {}, 'features': []}, u'Microsoft Comfort Mouse 4500': {'reviewCount': 118, 'product_id': u'B003BEDPHM', 'customer_service': {'sentiment': {}}, 'product_name': u'Microsoft Comfort Mouse 4500', 'issues': {}, 'features': []}}\n",
      "Samsung UN19F4000 19-Inch 720p 60Hz Slim LED HDTV\n",
      "164\n",
      "Bose QuietComfort 15 Acoustic Noise Cancelling Headphones\n",
      "804\n",
      "Microsoft Comfort Mouse 4500\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "from cloudant.client import Cloudant\n",
    "\n",
    "DB_USERNAME = '204f49bc-b226-413d-8dcf-aece9c16ce89-bluemix'\n",
    "DB_PASSWORD = 'b0527958a389317a174f7042c2b5edc94a786a0cf3e62fb83ae9dcb51b62dadd'\n",
    "DB_ACCOUNT = '204f49bc-b226-413d-8dcf-aece9c16ce89-bluemix'\n",
    "DATABASE = 'dbtest'\n",
    "\n",
    "client = Cloudant(DB_USERNAME, DB_PASSWORD, account=DB_ACCOUNT)\n",
    "client.connect()\n",
    "db = client[DATABASE]\n",
    "products = {}\n",
    "for doc in db:\n",
    "  if doc['title']:\n",
    "    if doc['title'] not in products:\n",
    "      products[doc['title']] = {}\n",
    "      products[doc['title']]['product_name'] = doc['title']\n",
    "      products[doc['title']]['customer_service'] = {'sentiment': {}}\n",
    "      products[doc['title']]['customer_service']['sentiment']['posCount'] = 0\n",
    "      products[doc['title']]['customer_service']['sentiment']['neuCount'] = 0\n",
    "      products[doc['title']]['customer_service']['sentiment']['negCount'] = 0\n",
    "      products[doc['title']]['features'] = []\n",
    "      products[doc['title']]['issues'] = {'count': 0}\n",
    "      products[doc['title']]['reviewCount'] = 0\n",
    "      products[doc['title']]['product_id'] = doc['asin']\n",
    "    \n",
    "    products[doc['title']]['reviewCount'] += 1\n",
    "    if doc['title']['sentiment'] == 'positive':\n",
    "      products[doc['title']]['customer_service']['sentiment']['posCount'] += 1\n",
    "    elif doc['title']['sentiment'] == 'neutral':\n",
    "      products[doc['title']]['customer_service']['sentiment']['neuCount'] += 1\n",
    "    elif doc['title']['sentiment'] == 'negative':\n",
    "      products[doc['title']]['customer_service']['sentiment']['negCount'] += 1\n",
    "    \n",
    "    #Add detected features list. I'm not sure how it's being stored during the alchemy call atm\n",
    "    #products[doc['title']]['features'].append(doc['title']['features'])\n",
    "    \n",
    "    #There is probably some other stuff that needs to be formatted here\n",
    "    #Until we can step through the whole process this is just kinda a guess\n",
    "    \n",
    "for product, value in products.items():\n",
    "    print product\n",
    "    #Need to push product to db whenever we're done here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "The script that runs the clustering on the review data is src/Processing/clustering.py\n",
    "A word2vec model is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cloudant.client import Cloudant\n",
    "from cloudant.query import Query\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "def generate_vectors(features, model):\n",
    "    vecs = []\n",
    "    mapping = []\n",
    "    count = 0\n",
    "    for line in features:\n",
    "        words = line['word'].split()\n",
    "        vec = []\n",
    "        flag = 0\n",
    "        for word in words:\n",
    "            word = str(word)\n",
    "            word = re.escape(word)\n",
    "            word = re.sub(r'\\\\', '', word)\n",
    "            if word in model:\n",
    "                if len(vec) > 1:\n",
    "                    vec = vec+model[word]\n",
    "                else:\n",
    "                    vec = model[word]\n",
    "            else:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            if len(vec) > 0:\n",
    "                vecs.append(vec)\n",
    "                mapping.append(count)\n",
    "        count += 1\n",
    "    return [vecs, mapping]\n",
    "\n",
    "\n",
    "def cluster_try(vecs):\n",
    "        clusterVec = {}\n",
    "        clusterIdx = {}\n",
    "        no_of_clusters = 1\n",
    "        clusterIdx[0] = [0]\n",
    "        clusterVec[0] = vecs[0]\n",
    "        max_sim = 0.5\n",
    "        index = 0\n",
    "        for i in range(1, len(vecs)):\n",
    "                flag = 0\n",
    "                max_sim = 0.5\n",
    "                for j in range(no_of_clusters):\n",
    "                        sim = np.dot(vecs[i], clusterVec[j])/(np.linalg.norm(clusterVec[j]) * np.linalg.norm(vecs[i]))\n",
    "                        if sim > max_sim:\n",
    "                                flag = 1\n",
    "                                max_sim = sim\n",
    "                                index = j\n",
    "                if flag == 0:\n",
    "                        clusterIdx[j+1] = [i]\n",
    "                        clusterVec[j+1] = vecs[i]\n",
    "                        no_of_clusters += 1\n",
    "                else:\n",
    "                        clusterIdx[index].append(i)\n",
    "                        clusterVec[index] += vecs[i]\n",
    "        return clusterIdx\n",
    "\n",
    "##I think this function can be deleted or at least drastically reduced given the change to groupings done earlier\n",
    "def create_json(clusters, cluster_data, mapping, keys, helpful, local_dump):\n",
    "    for i in clusters:\n",
    "        keyword_count = 0\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        neutral = 0\n",
    "        unique_words = {}\n",
    "        clusterinfo = {}\n",
    "        for key in clusters[i]:\n",
    "            index = mapping[key]\n",
    "            keyword = keys[index]['word']\n",
    "            if keyword in unique_words:\n",
    "                unique_words[keyword]['count'] += 1\n",
    "                unique_words[keyword]['review_id'].append(keys[index]['rev_id'])\n",
    "                unique_words[keyword]['sentence_id'].append(keys[index]['sentence_id'])\n",
    "            else:\n",
    "                unique_words[keyword] = {}\n",
    "                unique_words[keyword]['count'] = 1\n",
    "                unique_words[keyword]['review_id'] = [keys[index]['rev_id']]\n",
    "                unique_words[keyword]['sentence_id'] = [keys[index]['sentence_id']]\n",
    "            keyword_count += 1\n",
    "            list_keywords = []\n",
    "            stop_count = 0\n",
    "            for feature in sorted(unique_words, key=unique_words.get, reverse=True):\n",
    "                data = {}\n",
    "                data['keyword'] = feature\n",
    "                if stop_count == 0:\n",
    "                    clusterinfo['feature'] = feature\n",
    "                data['sentence_id'] = unique_words[feature]['sentence_id']\n",
    "                data['review_id'] = unique_words[feature]['review_id']\n",
    "                helpful_vote=0\n",
    "                for index_rev in range(0,len(data['review_id'])):\n",
    "                    if helpful[data['review_id'][index_rev]]>=helpful_vote:\n",
    "                        helpful_vote=helpful[data['review_id'][index_rev]]\n",
    "                        helpful_rev=index_rev\n",
    "\n",
    "                sent_id=data['sentence_id'][helpful_rev]\n",
    "                helpful_review=local_dump[data['review_id'][helpful_rev]]\n",
    "\n",
    "                ##cause of split reviews-to remove\n",
    "                sent_id=sent_id-helpful_review[0][0]['seqno']\n",
    "                if sent_id>0:\n",
    "                    excerpt=helpful_review[0][sent_id-1]['sentence']+helpful_review[0][sent_id]['sentence']\n",
    "                else:\n",
    "                    excerpt=helpful_review[0][sent_id]['sentence']\n",
    "                if sent_id<len(helpful_review[0])-1:\n",
    "                    excerpt=excerpt+helpful_review[0][sent_id+1]['sentence']\n",
    "\n",
    "                data['excerpt']=excerpt\n",
    "\n",
    "                data['count'] = unique_words[feature]['count']\n",
    "                list_keywords.append(data)\n",
    "                stop_count += 1\n",
    "                if stop_count == 3:\n",
    "                    break\n",
    "            if keys[index]['sentiment'][0][0] == 'positive':\n",
    "                    pos += 1\n",
    "            if keys[index]['sentiment'][0][0] == 'neutral':\n",
    "                    neutral += 1\n",
    "            if keys[index]['sentiment'][0][0] == 'negative':\n",
    "                    neg += 1\n",
    "        clusterinfo['keywords'] = list_keywords\n",
    "        clusterinfo['sentiments'] = {}\n",
    "        clusterinfo['sentiments']['positive'] = pos\n",
    "        clusterinfo['sentiments']['negative'] = neg\n",
    "        clusterinfo['sentiments']['neutral'] = neutral\n",
    "        clusterinfo['keyword_count'] = keyword_count\n",
    "        cluster_data.append(clusterinfo)\n",
    "    return cluster_data\n",
    "\n",
    "\n",
    "def cluster(doc, db, asin):\n",
    "    \n",
    "    #Since we did grouping in the previous step this shouldn't be necessary\n",
    "    ####start###############\n",
    "    SERVER = 'https://1790ef54-fcf2-4029-9b73-9000dff88e6e-bluemix.cloudant.com'\n",
    "    DATABASE = 'amazon_data'\n",
    "    USERNAME = '1790ef54-fcf2-4029-9b73-9000dff88e6e-bluemix'\n",
    "    PASSWORD = '5beb3f8b9f95586542e3d9c5acfb0c52832252432623e534d4e88b12fad29638'\n",
    "\n",
    "    server = cloudant.client.Cloudant(USERNAME, PASSWORD, url=SERVER)\n",
    "    server.connect()\n",
    "    db = server[DATABASE]\n",
    "    query = Query(db, selector={'asin': asin, 'type':['review']},fields=[\"_id\", \"helpful\"])\n",
    "    meta = Query(db, selector={'asin': asin, 'type': ['metadata']})\n",
    "    meta = meta.result[0][0]\n",
    "    name = ''\n",
    "    if 'title' in meta:\n",
    "        name = meta['title']\n",
    "    rev_id = []\n",
    "    helpful={}\n",
    "    for data in query.result:\n",
    "        rev_id.append(data['_id'])\n",
    "        if 'helpful' in data:\n",
    "            helpful[data['_id']]=data['helpful'][0]\n",
    "        else:\n",
    "            helpful[data['_id']]=0\n",
    "\n",
    "    temp = {}\n",
    "    keys = []\n",
    "    local_dump = {}\n",
    "    for rev in rev_id:\n",
    "        query_id = Query(db, selector={'review_id': rev, 'type': ['classified']})\n",
    "        for i in query_id.result:\n",
    "            if len(query_id.result[0]) == 0:\n",
    "                continue\n",
    "        for res in query_id.result[0]:\n",
    "            text = res['review']\n",
    "            local_dump[res['review_id']] = text\n",
    "            for obj in text[0]:\n",
    "                if 'Feature' in obj:\n",
    "                    feature = obj['Feature']\n",
    "                    for data in feature:\n",
    "                        if 'name' in data:\n",
    "                            temp = {}\n",
    "                            temp['word'] = data['name']\n",
    "                            if 'sentiment' in data:\n",
    "                                temp['sentiment'] = data['sentiment']\n",
    "                            else:\n",
    "                                temp['sentiment'] = ['neutral']\n",
    "                            temp['rev_id'] = res['review_id']\n",
    "                            temp['sentence_id'] = obj['seqno']\n",
    "                            keys.append(temp)\n",
    "    modelname = 'sample_model'\n",
    "    cwd = os.getcwd()\n",
    "    model = word2vec.Word2Vec.load_word2vec_format(cwd+ '/' + modelname+'.bin', binary=True)\n",
    "    [vecs, mapping] = generate_vectors(keys, model)\n",
    "    clusters = cluster_try(vecs)\n",
    "    cluster_data = []\n",
    "    features = create_json(clusters, cluster_data, mapping, keys, helpful, local_dump)\n",
    "    features = sorted(features, key=lambda k: k['keyword_count'], reverse=True)\n",
    "\n",
    "    featureDict = {}\n",
    "    featureDict['features'] = features[:10]\n",
    "    featureDict['product_name'] = name\n",
    "    ###end#############\n",
    "    \n",
    "    \n",
    "    #I think it should look something like this instead\n",
    "    ###start######\n",
    "\n",
    "    #remove variables\n",
    "    DB_USERNAME = '204f49bc-b226-413d-8dcf-aece9c16ce89-bluemix'\n",
    "    DB_PASSWORD = 'b0527958a389317a174f7042c2b5edc94a786a0cf3e62fb83ae9dcb51b62dadd'\n",
    "    DB_ACCOUNT = '204f49bc-b226-413d-8dcf-aece9c16ce89-bluemix'\n",
    "    DATABASE = 'dbtest'\n",
    "\n",
    "    client = Cloudant(DB_USERNAME, DB_PASSWORD, account=DB_ACCOUNT)\n",
    "    client.connect()\n",
    "    db = client[DATABASE]\n",
    "    \n",
    "    modelname = 'sample_model'\n",
    "    cwd = os.getcwd()\n",
    "    model = word2vec.Word2Vec.load_word2vec_format(cwd+ '/' + modelname+'.bin', binary=True)\n",
    "    for doc in db:\n",
    "      if doc['product_name']:\n",
    "        [vecs, mapping] = generate_vectors(doc['features'], model)\n",
    "        clusters = cluster_try(vecs)\n",
    "        #update db with the clusters\n",
    "    ###end########\n",
    "\n",
    "\n",
    "    return featureDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Final JSON\n",
    "The script that takes all of these pieces and turns them into the final JSON structure that will be used by a front end application is src/Processing/makeFinalJSON.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We might be able to skip this whole step. I did the grouping in an earlier section to make the clustering step simpler.\n",
    "#If we get that formatted correctly there then I think this can be removed. -Andrew\n",
    "\n",
    "from watson_developer_cloud import AlchemyLanguageV1\n",
    "from cloudant.query import Query\n",
    "from cloudant.client import Cloudant\n",
    "\n",
    "\n",
    "JSON_FILE = \"./Model_Clustering.JSON\"\n",
    "outputJSON = {\n",
    "    \"product_name\": \"\",\n",
    "    \"product_id\": None,\n",
    "    \"features\": [],\n",
    "    \"issues\": {\n",
    "        \"percentage\": 0,\n",
    "        \"review_ids\": []\n",
    "    },\n",
    "    \"customer_service\": {\n",
    "        \"sentiment\": {\n",
    "            \"positive\": 0,\n",
    "            \"neutral\": 0,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "DB_USERNAME = ''\n",
    "DB_PASSWORD = ''\n",
    "DB_ACCOUNT = ''\n",
    "DATABASE = ''                                               \n",
    "AL_KEY = ''\n",
    "\n",
    "client = Cloudant(DB_USERNAME, DB_PASSWORD, account=DB_ACCOUNT)\n",
    "client.connect()\n",
    "db = client[DATABASE]\n",
    "\n",
    "def make_final(cluster, db):\n",
    "    outputJSON = {\"product_name\": \"\",\"product_id\": None,\"features\": [],\"issues\": {\"percentage\": 0,\"review_ids\": []},\"customer_service\": {\"sentiment\": {\"positive\": 0,\"neutral\": 0,\"negative\": 0}}}\n",
    "    alchemy = AlchemyLanguageV1(api_key=AL_KEY)\n",
    "\n",
    "    reviewnums = set()\n",
    "    reviews = []\n",
    "\n",
    "    for group in cluster['features']:\n",
    "        for keywords in group['keywords']:\n",
    "            for review_ids in keywords['review_id']:\n",
    "                reviewnums.add(review_ids)\n",
    "    for i in range(0, len(reviewnums)):\n",
    "        if len(reviewnums) > 0:\n",
    "            num = reviewnums.pop()\n",
    "            q = Query(db, selector={'review_id': num})\n",
    "            for i in q.result():\n",
    "                if i['type'] == ['classified']:\n",
    "                    reviews.append(q.result[0])\n",
    "    reviewnums.clear()\n",
    "    total = 0\n",
    "    for review in reviews:\n",
    "        if review != []:\n",
    "            for line in review[0][\"review\"]:\n",
    "                total = total + 1\n",
    "                if type(line) == list:\n",
    "                    line = line[0]\n",
    "                if type(line) == int:\n",
    "                    continue\n",
    "                if(line[\"layer3type\"] == \"Issue\"):\n",
    "                    outputJSON[\"issues\"][\"percentage\"] = outputJSON[\"issues\"][\"percentage\"] + 1\n",
    "                    outputJSON[\"issues\"][\"review_ids\"].append(review[0][\"review_id\"])\n",
    "                if(line[\"layer2type\"] == \"Customer Service\"):\n",
    "                    sentiment = alchemy.sentiment(text=line[\"sentence\"])[\"docSentiment\"][\"type\"]\n",
    "                    outputJSON[\"customer_service\"][\"sentiment\"][sentiment] = outputJSON[\"customer_service\"][\"sentiment\"][sentiment] + 1\n",
    "\n",
    "    outputJSON[\"issues\"][\"percentage\"] = outputJSON[\"issues\"][\"percentage\"]/float(total)*100\n",
    "    customer_service_total = 0\n",
    "    for sentiment in outputJSON[\"customer_service\"][\"sentiment\"]:\n",
    "        customer_service_total = customer_service_total + outputJSON[\"customer_service\"][\"sentiment\"][sentiment]\n",
    "\n",
    "    if(customer_service_total > 0):\n",
    "        for sentiment in outputJSON[\"customer_service\"][\"sentiment\"]:\n",
    "            outputJSON[\"customer_service\"][\"sentiment\"][sentiment] = outputJSON[\"customer_service\"][\"sentiment\"][sentiment]/float(customer_service_total)*100\n",
    "\n",
    "    featureArray = cluster[\"features\"]\n",
    "    total = 0\n",
    "    for item in featureArray:\n",
    "        feature = {}\n",
    "        feature[\"group_name\"] = item[\"feature\"]\n",
    "        feature[\"percentage\"] = item[\"keyword_count\"]\n",
    "        total = total + item[\"keyword_count\"]\n",
    "        feature[\"sentiments\"] = item[\"sentiments\"]\n",
    "        feature[\"keywords\"] = []\n",
    "        for i in range(len(item[\"keywords\"])):\n",
    "            elem = item[\"keywords\"][i]\n",
    "            keyword = {\n",
    "                \"name\": elem[\"keyword\"],\n",
    "                \"review_id\": elem[\"review_id\"],\n",
    "                \"sentence_id\": elem[\"sentence_id\"]\n",
    "            }\n",
    "            feature[\"keywords\"].append(keyword)\n",
    "        for sent in feature[\"sentiments\"]:\n",
    "            feature[\"sentiments\"][sent] = feature[\"sentiments\"][sent]/float(item[\"keyword_count\"])*100\n",
    "        outputJSON[\"features\"].append(feature)\n",
    "\n",
    "    for item in outputJSON[\"features\"]:\n",
    "        item[\"percentage\"] = item[\"percentage\"]/float(total)*100\n",
    "\n",
    "    return outputJSON\n",
    "\n",
    "client = cloudant.client.Cloudant(DB_USERNAME, DB_PASSWORD, account=DB_ACCOUNT)\n",
    "client.connect()\n",
    "db = client[DATABASE]\n",
    "for doc in db:\n",
    "  doc['class'] = classify(doc['reviewText'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
