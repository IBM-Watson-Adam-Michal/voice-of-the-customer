{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data\n",
    "Take your data and upload to Cloudant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Data into CSV\n",
    "The data in the database must be converted into a form that can be used to train the models for entity extraction and natural language classification. To do this, run the script below. Please edit the database connections so that they point to your database where all your data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cloudant\n",
    "import csv\n",
    "\n",
    "SERVER = 'https://1790ef54-fcf2-4029-9b73-9000dff88e6e-bluemix.cloudant.com' #Replace with your server URL\n",
    "DATABASE = 'amazon_data' #Replace with the name of the database\n",
    "USERNAME = '1790ef54-fcf2-4029-9b73-9000dff88e6e-bluemix' #Replace with the username from your credentials for the NLC\n",
    "PASSWORD = '5beb3f8b9f95586542e3d9c5acfb0c52832252432623e534d4e88b12fad29638' #Replace with the password from your credentials for the NLC\n",
    "DESIGN = '' #replace with the name of the design document that contains the view. This should be of the form '_design/XXXX'\n",
    "VIEW = 'names/namesAndAsin' #Replace with the view from your database to poll, this should take the form of view_file/view and should return the text to classify as the value field and what you would like to call it as the key\n",
    "DESTINATION = 'out.csv' #Replace with correct name for output file (NOTE must be *.csv)\n",
    "\n",
    "server = cloudant.client.Cloudant(USERNAME,PASSWORD,url=SERVER)\n",
    "server().connect\n",
    "db = server[DATABASE]\n",
    "query = db.get_view_result(DESIGN,VIEW)\n",
    "file = open(DESTINATION, 'wb')\n",
    "writer = csv.writer(file)\n",
    "\n",
    "\n",
    "for q in query:\n",
    "    print q[0]\n",
    "    if 'key' in q[0] and q[0]['key'] != None:\n",
    "        title = q[0]['key']\n",
    "    else:\n",
    "        title = \"No Title\"\n",
    "    if 'value' in q[0] and q[0]['value'] != None:\n",
    "        text = q[0]['value']\n",
    "    else:\n",
    "        text = \"No Text\"\n",
    "    writer.writerow([title,text])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data CSV into Watson Knowledge Studio\n",
    "The CSV that is created at the end of the script must now be imported into Watson Knowledge Studio (WKS). Each row in the CSV will be treated as a separate document and will be organized in WKS. The documents then need to be annotated with entities and relationships. Coreference is also done here. A strict guideline is very helpful when doing this.\n",
    "\n",
    "After the annotations are done and the model is trained, it needs to be exported into Alchemy Language using an API key. This API key is used below in the url variable. Please replace it in the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import ast\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6)\n",
    "\n",
    "def get_relations(review):\n",
    "    url = \"http://access.alchemyapi.com/calls/text/TextGetTypedRelations?showSourceText=1&model=8f03f142-af7f-4487-be47-371fc3262705&apikey=ffd7397f4be657f7740a84038f903271b2707a11&outputMode=json\"\n",
    "    f = requests.get(url, params={'text':review})\n",
    "    response = f.content\n",
    "    response = ast.literal_eval(response)\n",
    "    return response\n",
    "\n",
    "def get_entities(review)\n",
    "    url = \"http://access.alchemyapi.com/calls/text/TextGetRankedNamedEntities?showSourceText=1&model=8f03f142-af7f-4487-be47-371fc3262705&apikey=ffd7397f4be657f7740a84038f903271b2707a11&outputMode=json&sentiment=1\"\n",
    "    f = requests.get(url, params={'text':review})\n",
    "    response = f.content\n",
    "    response = ast.literal_eval(response)\n",
    "    return response\n",
    "\n",
    "def token_replacement_entities(reivew):\n",
    "    review = get_entities(review)\n",
    "    if 'entities' in review:\n",
    "        entities = review['entities']\n",
    "        text = review['text']\n",
    "        for i in entities:\n",
    "            token = i['text']\n",
    "            count = i['count']\n",
    "            classification = \"<\" + i['type'] + \">\"\n",
    "            text = text.replace(token,classification,count)\n",
    "    return text\n",
    "\n",
    "def token_replacement_relations(review):\n",
    "    review = get_relations(review)\n",
    "    if 'typedRelations' in review:\n",
    "        types = review['typedRelations']\n",
    "        text = review['text']\n",
    "        standard = \"\"\n",
    "        previous = \"\"\n",
    "        new_sentence = True\n",
    "        review_text_mod = \"\"\n",
    "        flip = True\n",
    "        sentence_dict={}\n",
    "        prev_dict={}\n",
    "        if types != []:\n",
    "            count=0\n",
    "            result=[]\n",
    "            for sentence in types:\n",
    "                #dict={}\n",
    "                review_text = sentence['sentence']\n",
    "                #dict['sentence']=review_text\n",
    "                if review_text != standard:\n",
    "                    previous = standard\n",
    "                    standard = review_text\n",
    "                    new_sentence = True\n",
    "                    dict={}\n",
    "                    sentence_dict[standard]=count\n",
    "                    dict['sentence']=standard\n",
    "                    dict['seqno']=count\n",
    "                    print \"\\ndict\\n\"\n",
    "                    print dict\n",
    "                    text = text.replace(previous, review_text_mod, 1)\n",
    "                    count+=1\n",
    "                    #if count>1:\n",
    "                    result.append(prev_dict)\n",
    "                else:\n",
    "                    new_sentence = False\n",
    "                    dict=result[sentence_dict[review_text]]\n",
    "                    review_text = review_text_mod\n",
    "                temp_dict={}\n",
    "                temp_dict['hasrel']=sentence['type']\n",
    "                for entity in sentence['arguments']:\n",
    "                    for i in entity['entities']:\n",
    "                        token = i['text']\n",
    "                        temp_dict[entity['part']]=token\n",
    "                        if entity['part']=='first':\n",
    "                            if i['type'] in dict:\n",
    "                                dict[i['type']].append(temp_dict)\n",
    "                            else:\n",
    "                                dict[i['type']]=[]\n",
    "                                dict[i['type']].append(temp_dict)\n",
    "                        else:\n",
    "                            sec_dict={}\n",
    "                            sec_dict['name']=i['text']\n",
    "                            if i['type'] in dict:\n",
    "                                dict[i['type']].append(sec_dict)\n",
    "                            else:\n",
    "                                dict[i['type']]=[]\n",
    "                                dict[i['type']].append(sec_dict)\n",
    "                        classification = \"<\" + i['type'] + \">\"\n",
    "                        if flip:\n",
    "                            review_text_mod = review_text.replace(token,classification,1)\n",
    "                            flip = False\n",
    "                        else:\n",
    "                            review_text_mod = review_text_mod.replace(token,classification,1)\n",
    "                            flip = True\n",
    "                dict['reversed_sentence']=review_text_mod\n",
    "                #print temp_dict\n",
    "                print \"final dict\"\n",
    "                print dict\n",
    "                prev_dict=dict\n",
    "                result[sentence_dict[sentence['sentence']]]=dict\n",
    "                #result.append(dict)\n",
    "                print \"new\"\n",
    "                #print sentence_dict\n",
    "        pp.pprint(result)\n",
    "        text = text.replace(standard,review_text_mod,1)\n",
    "        return text\n",
    "        #pp.print(result\n",
    "    else: \n",
    "        return review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace entities with Tags in Training Sets\n",
    "You will need to run your training set CSV through the script below which replaces any entities found by Entity Extraction in Alchemy language with a representative tag in order generalize the classifier in the coming steps. Just identify the name for your csv file input and output and it should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import token_replacement as t\n",
    "import csv\n",
    "\n",
    "read = open('ground_truth_layer1.csv','rb') ##replace with correct filenames\n",
    "write = open('ground_truth_layer1_replace.csv','wb')     ##replace with correct filenames\n",
    "\n",
    "reader = csv.reader(read)\n",
    "writer = csv.writer(write)\n",
    "\n",
    "for row in reader:\n",
    "    token = t.token_replacement(row[0])\n",
    "    writer.writerow([token,row[1]])\n",
    "\n",
    "read.close()\n",
    "write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Natural Language Classifier\n",
    "The Natural Language Classifier (NLC) needs to be trained on this generalized/tagged data. This is a 3 level NLC that will provide better classification results than just using a single layer. Add your credentials in the file below and run the script to train the NLC. The script will also store your classifier_id's in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import couchdbkit\n",
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "import csv\n",
    "\n",
    "tier1CSV = 'training_set1.csv'\n",
    "tier2CSV = 'training_set2.csv'\n",
    "tier3CSV = 'training_set3.csv'\n",
    "USERNAME = 'e561bc30-d294-41f4-8b47-39fc6bc29917'\n",
    "PASSWORD = 'XH8pYnsYfClv'\n",
    "JSON_TARGET = '../../data/classifier_ids.json'\n",
    "\n",
    "classifierTree = {\n",
    "    'tier1':'',\n",
    "    'tier2':'',\n",
    "    'tier3':''\n",
    "}\n",
    "\n",
    "# Initialize classifier\n",
    "nlc = NaturalLanguageClassifierV1(username = USERNAME, password = PASSWORD)\n",
    "\n",
    "# Train tier 1 classifier\n",
    "print(\"############# TIER 1 CLASSIFIER ##############\")\n",
    "with open(tier1CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier1',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier1'] = classifier['classifier_id']\n",
    "\n",
    "# Train tier 2 classifier\n",
    "print(\"############# TIER 2 CLASSIFIER ##############\")\n",
    "with open(tier2CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier2',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier2'] = classifier['classifier_id']\n",
    "\n",
    "# Train tier 3 classifier\n",
    "print(\"############# TIER 3 CLASSIFIER ##############\")\n",
    "with open(tier3CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier3',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier3'] = classifier['classifier_id']\n",
    "\n",
    "# Write the tiers with classifier id's to file for use later\n",
    "with open(JSON_TARGET, 'w') as outfile:\n",
    "    json.dump(classifierTree, outfile)\n",
    "\n",
    "print(\"############# FULL CLASSIFIER TREE ##############\")\n",
    "print(json.dumps(classifierTree))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
