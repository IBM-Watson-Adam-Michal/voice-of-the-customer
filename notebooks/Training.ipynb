{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data\n",
    "The first step is to find a data set that needs to be analyzed. The data needs to be stored so that the rest of the flow can use it. It is recommended to use a database and store the data in JSON format but any way of saving the data is fine as long as it can be accessed throughout the flow. Examples include just storing it in a JSON file, SQL database, NoSQL database, CSV file, etc. For the purposes of this starter kit, the data is being stored in a Cloudant NoSQL database (available on bluemix) and that is the simplest way to get started and requires the fewest changes to be made to the scripts. We offer a few scripts to help push and pull from this database to make it even easier.\n",
    "\n",
    "The minimum amount of data needed is review/feedback text and a way to link it to the product it is targeted to (most likely with some sort of key/id to mark a review for a product).\n",
    "\n",
    "Take the dataset and push it to your database/file in a format that is easiest to index and use further in the flow. For reference, here is an example of a review that we used: \n",
    "\n",
    "TODO: ADD JSON STRING HERE\n",
    "\n",
    "Key things to note: By storing it this way, it is easy to retrieve the text of the review and find out which product it is pointed to so that the rest of the flow can be easily executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Data into CSV\n",
    "The data must be converted into a form that can be used to train the models for entity extraction. To do this, run the script below. Please edit the database connections so that they point to your database where all your data is stored or recode that portion so that it points to wherever the data is located (if not in a cloudant database).\n",
    "\n",
    "The important portion is the output and the format of the output csv that is ingested by Watson Knowledge Studio is:\n",
    "\n",
    "\"key\",\"value\"\n",
    "\"TITLE OF DOCUMENT\",\"TEXT OF DOCUMENT\"\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data CSV into Watson Knowledge Studio\n",
    "The data must be converted into a form that can be used to train the models for entity extraction through Watson Knowledge Studio. To do this, run the script below. Please edit the database connections so that they point to your database where all your data is stored or recode that portion so that it points to wherever the data is located (if not in a cloudant database).\n",
    "\n",
    "The important portion is the output and the format of the output csv that is ingested by Watson Knowledge Studio is:\n",
    "\n",
    "\"key\",\"value\"\n",
    "\"TITLE OF DOCUMENT\",\"TEXT OF DOCUMENT\"\n",
    "...\n",
    "\n",
    "The CSV that is created at the end of the script must now be imported into Watson Knowledge Studio (WKS). Each row in the CSV will be treated as a separate document and will be organized in WKS. The data doesn't have to be in CSV format to be uploaded into WKS, one can also do it manually by uploading documents but we provide a tool to convert it into CSV to make it easier.\n",
    "\n",
    "The documents then need to be annotated with entities and relationships. Coreference is also done here. A strict guideline is very helpful when doing this.\n",
    "\n",
    "For guidelines and tips on Watson Knowledge Studio, reference /notebooks/WKS.md\n",
    "\n",
    "After the annotations are done and the model is trained, it needs to be exported into Alchemy Language using an API key. To do this, first get an Alchemy API key from bluemix and then reference the WKS documentation above to see how that is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#db2file.py\n",
    "\n",
    "import cloudant\n",
    "import csv\n",
    "\n",
    "SERVER = ''      ''' Replace with your server URL'''\n",
    "DATABASE = ''    ''' Replace with the name of the database'''\n",
    "USERNAME = ''    ''' Replace with the username from your\n",
    "                    credentials for the NLC'''\n",
    "PASSWORD = ''    ''' Replace with the password from\n",
    "                    your credentials for the NLC'''\n",
    "DESIGN = ''      ''' Replace with the name of the design document that contains\n",
    "                     the view. This should be of the form '_design/XXXX''''\n",
    "VIEW = ''        ''' Replace with the view from your database to poll,\n",
    "                    this should take the form of view_file/view and should\n",
    "                    return the text to classify as the value field and what\n",
    "                    you would like to call it as the key'''\n",
    "DESTINATION = ''  ''' Replace with correct name for output\n",
    "                    file (NOTE must be *.csv)'''\n",
    "\n",
    "server = cloudant.client.Cloudant(USERNAME, PASSWORD, url=SERVER)\n",
    "server.connect()\n",
    "db = server[DATABASE]\n",
    "query = db.get_view_result(DESIGN, VIEW)\n",
    "file = open(DESTINATION, 'wb')\n",
    "writer = csv.writer(file)\n",
    "\n",
    "\n",
    "for q in query:\n",
    "    print q[0]\n",
    "    if 'key' in q[0] and q[0]['key'] is not None:\n",
    "        title = q[0]['key']\n",
    "    else:\n",
    "        title = \"No Title\"\n",
    "    if 'value' in q[0] and q[0]['value'] is not None:\n",
    "        text = q[0]['value']\n",
    "    else:\n",
    "        text = \"No Text\"\n",
    "    writer.writerow([title, text])\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and testing sets\n",
    "In order to train and validate your Natural Language Classifier, a training and testing set must be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training_testing.py\n",
    "\n",
    "'''\n",
    "This script creates a training and testing split out of your data for you. It\n",
    "can either be run interactivly by running the script witout arguments, or run\n",
    "automatically with command line arguments.\n",
    "The flags are:\n",
    "    -l The location of the data, a directory of .txt or .json files or a .csv\n",
    "        file\n",
    "    -r Percentage of data to split into training\n",
    "    -e Precentage of data to split into testing\n",
    "    -j Field in .json that contains the text data. Only necessary if loading\n",
    "        from a .json file. \n",
    "'''\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import getopt\n",
    "\n",
    "\n",
    "def csv_handler(file, training, testing):\n",
    "    ftest = open(\"testing_set.csv\", \"wb\")\n",
    "    ftrain = open(\"training_set.csv\", \"wb\")\n",
    "    wtest = csv.writer(ftest)\n",
    "    wtrain = csv.writer(ftrain)\n",
    "\n",
    "    rand = np.random.rand\n",
    "\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        if rand() < training:\n",
    "            wtrain.writerow([row])\n",
    "        else:\n",
    "            wtest.writerow([row])\n",
    "\n",
    "    ftest.close()\n",
    "    ftrain.close()\n",
    "\n",
    "\n",
    "def txt_handler(file, writer):\n",
    "\n",
    "    text = file.read()\n",
    "    writer.writerow([text])\n",
    "\n",
    "\n",
    "def json_handler(file, writer, json_field):\n",
    "    raw_text = file.read()\n",
    "\n",
    "    try:\n",
    "        processed_text = ast.literal_eval(raw_text)\n",
    "        text = processed_text[json_field]\n",
    "        writer.writerow([text])\n",
    "    except:\n",
    "        print \"ERROR: Something wrong with .json file: \" + file.name\n",
    "flags = {}\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    args = sys.argv[1:]\n",
    "    opts = getopt.getopt(args, 'l:e:r:j:')\n",
    "    for pair in opts:\n",
    "        flags[pair[0]] = pair[1]\n",
    "\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    print \"Input the full path to your data\"\n",
    "    print \"The data can be in the format of a .csv file (with one column\" + \\\n",
    "        \" and one text per line), or a directory of .json or .txt files.\"\n",
    "    print \"NOTE: If you are using a directory, please make sure your data\" + \\\n",
    "        \"is the only thing in the directory\"\n",
    "    location = raw_input(\"Data Location: \")\n",
    "else:\n",
    "    if '-l' in flags:\n",
    "        location = flags['-l']\n",
    "    else:\n",
    "        print \"ERROR: No file location. Did you use the -l\" +\\\n",
    "            \" flag to mark a file location?\"\n",
    "\n",
    "if re.match(r\".\\.csv$\", location):\n",
    "    try:\n",
    "        f = open(location, 'rb')\n",
    "        training = 0\n",
    "        testing = 0\n",
    "        if len(sys.argv) > 1:\n",
    "            while training + testing != 100:\n",
    "                print \"What fraction would you like to use for training?\" + \\\n",
    "                    \" (We recommend 70%)\"\n",
    "                training = raw_input(\"Training (0-100): \")\n",
    "                print \"What fraction would you like to use for testing?\" + \\\n",
    "                    \" (We recommend 30%)\"\n",
    "                testing = raw_input(\"Testing (0-100): \")\n",
    "                if training + testing != 100:\n",
    "                    print \"ERROR: Training and testing sets must equal 100%\"\n",
    "        else:\n",
    "            if '-r' in flags and '-e' in flags:\n",
    "                training = flags['-r']\n",
    "                testing = flags['-e']\n",
    "            else:\n",
    "                print \"ERROR: No training or testing split.\" + \\\n",
    "                    \" Did you use the -r and -e flags to mark them?\"\n",
    "\n",
    "        training = float(training)/100\n",
    "        testing = float(testing)/100\n",
    "\n",
    "        csv_handler(f, training, testing)\n",
    "        f.close()\n",
    "\n",
    "    except (IOError):\n",
    "        print \"ERROR: File not found\"\n",
    "\n",
    "else:\n",
    "    json_field = \"\"\n",
    "    try:\n",
    "        files = os.listdir(location)\n",
    "        rand = np.random.rand\n",
    "        total_docs = len(files)\n",
    "        training = 0\n",
    "        testing = 0\n",
    "        if len(sys.argv) > 1:\n",
    "            while training + testing != 100:\n",
    "                print \"What fraction would you like to use for training?\" + \\\n",
    "                    \" (We recommend 70%)\"\n",
    "                training = input(\"Training (0-100): \")\n",
    "                print \"What fraction would you like to use for testing?\" + \\\n",
    "                    \" (We recommend 30%)\"\n",
    "                testing = input(\"Testing (0-100): \")\n",
    "                if training + testing != 100:\n",
    "                    print \"ERROR: Training and testing sets must equal 100%\"\n",
    "        else:\n",
    "            if '-r' in flags and '-e' in flags:\n",
    "                training = flags['-r']\n",
    "                testing = flags['-e']\n",
    "            else:\n",
    "                print \"ERROR: No training or testing split.\" + \\\n",
    "                    \" Did you use the -r and -e flags to mark them?\"\n",
    "\n",
    "        training = float(training)/100\n",
    "        testing = float(testing)/100\n",
    "\n",
    "        ftest = open(\"testing_set.csv\", \"wb\")\n",
    "        ftrain = open(\"training_set.csv\", \"wb\")\n",
    "        wtest = csv.writer(ftest)\n",
    "        wtrain = csv.writer(ftrain)\n",
    "\n",
    "        for entry in files:\n",
    "            if re.match(r\".\\.txt$\", entry):\n",
    "                f = open(location + '/' + entry, 'rb')\n",
    "\n",
    "                if rand() < training:\n",
    "                    txt_handler(f, wtrain)\n",
    "                else:\n",
    "                    txt_handler(f, wtest)\n",
    "                f.close()\n",
    "            if re.match(r\".\\.json$\", entry):\n",
    "                if len(sys.argv) > 1:\n",
    "                    if json_field == \"\":\n",
    "                        print \"What key in the .json contains your text data?\"\n",
    "                        json_field = raw_input(\"Json Key: \")\n",
    "                else:\n",
    "                    if '-j' in flags:\n",
    "                        json_fields = flags['-j']\n",
    "                    if json_field == \"\":\n",
    "                        print \"Please use the -j flag to give the key of\" + \\\n",
    "                            \"the .json that contains the text data.\"\n",
    "\n",
    "                f = open(location + '/' + entry, 'rb')\n",
    "\n",
    "                if rand() < training:\n",
    "                    json_handler(f, wtrain, json_field)\n",
    "                else:\n",
    "                    json_handler(f, wtest, json_field)\n",
    "                f.close()\n",
    "        ftest.close()\n",
    "        ftrain.close()\n",
    "    except OSError:\n",
    "        print \"ERROR: Directory not found\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Natural Language Classifier\n",
    "The Natural Language Classifier (NLC) needs to be trained on this generalized/tagged data. This is a 3 level NLC that will provide better classification results than just using a single layer. Add your credentials in the file below and run the script to train the NLC. The script will also store your classifier_id's in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import couchdbkit\n",
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "import csv\n",
    "\n",
    "tier1CSV = 'training_set1.csv'\n",
    "tier2CSV = 'training_set2.csv'\n",
    "tier3CSV = 'training_set3.csv'\n",
    "USERNAME = 'e561bc30-d294-41f4-8b47-39fc6bc29917'\n",
    "PASSWORD = 'XH8pYnsYfClv'\n",
    "JSON_TARGET = '../../data/classifier_ids.json'\n",
    "\n",
    "classifierTree = {\n",
    "    'tier1':'',\n",
    "    'tier2':'',\n",
    "    'tier3':''\n",
    "}\n",
    "\n",
    "# Initialize classifier\n",
    "nlc = NaturalLanguageClassifierV1(username = USERNAME, password = PASSWORD)\n",
    "\n",
    "# Train tier 1 classifier\n",
    "print(\"############# TIER 1 CLASSIFIER ##############\")\n",
    "with open(tier1CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier1',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier1'] = classifier['classifier_id']\n",
    "\n",
    "# Train tier 2 classifier\n",
    "print(\"############# TIER 2 CLASSIFIER ##############\")\n",
    "with open(tier2CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier2',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier2'] = classifier['classifier_id']\n",
    "\n",
    "# Train tier 3 classifier\n",
    "print(\"############# TIER 3 CLASSIFIER ##############\")\n",
    "with open(tier3CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier3',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier3'] = classifier['classifier_id']\n",
    "\n",
    "# Write the tiers with classifier id's to file for use later\n",
    "with open(JSON_TARGET, 'w') as outfile:\n",
    "    json.dump(classifierTree, outfile)\n",
    "\n",
    "print(\"############# FULL CLASSIFIER TREE ##############\")\n",
    "print(json.dumps(classifierTree))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
