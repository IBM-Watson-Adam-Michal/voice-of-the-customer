{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gathering data\n",
    "The first step is to find a data set that needs to be analyzed. The data needs to be stored so that the rest of the flow can use it. It is recommended to use a database and store the data in JSON format but any way of saving the data is fine as long as it can be accessed throughout the flow. Examples include just storing it in a JSON file, SQL database, NoSQL database, CSV file, etc. For the purposes of this starter kit, the data is being stored in a Cloudant NoSQL database (available on Bluemix http://bluemix.net/) and that is the simplest way to get started and requires the fewest changes to be made to the scripts. We offer a few scripts to help push and pull from this database to make it even easier.\n",
    "\n",
    "The minimum amount of data needed is some review/feedback text and a way to link it to the product it is targeted to (most likely with some sort of key/id to mark a review for a product).\n",
    "\n",
    "Take the dataset and push it to your database/file in a format that is easiest to index and use further in the flow. For reference, here is an example of a review that we used: \n",
    "\n",
    "{\n",
    " \"reviewerID\": \"AO94DHGC771SJ\", \n",
    " \"asin\": \"0528881469\", \n",
    " \"reviewerName\": \"amazdnu\", \n",
    " \"helpful\": [0, 0], \n",
    " \"reviewText\": \"We got this GPS for my husband who is an (OTR) over the road trucker.  Very Impressed with the shipping time, it arrived a few days earlier than expected...  within a week of use however it started freezing up... could of just been a glitch in that unit.  Worked great when it worked!  Will work great for the normal person as well but does have the \\\"trucker\\\" option. (the big truck routes - tells you when a scale is coming up ect...)  Love the bigger screen, the ease of use, the ease of putting addresses into memory.  Nothing really bad to say about the unit with the exception of it freezing which is probably one in a million and that's just my luck.  I contacted the seller and within minutes of my email I received a email back with instructions for an exchange! VERY impressed all the way around!\", \n",
    " \"overall\": 5.0, \n",
    " \"summary\": \"Gotta have GPS!\", \n",
    " \"unixReviewTime\": 1370131200, \n",
    " \"reviewTime\": \"06 2, 2013\"\n",
    "}\n",
    "\n",
    "The data was downloaded from http://jmcauley.ucsd.edu/data/amazon/ and the Electronics section was the one used for the demo.\n",
    "\n",
    "Key things to note: By storing it this way, it is easy to retrieve the text of the review and find out which product it is pointed to so that the rest of the flow can be easily executed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing data into Cloudant\n",
    "\n",
    "In order to preprocess the data, reviews used in this example are uploaded to a Cloudant database. Assuming you have followed instructions for setting up a Cloudant service under your Bluemix account and have added its credentials to your local .env file, the script below should upload the data you have to your Cloudant database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import couchdbkit\n",
    "import configparser\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "logger.debug(config.sections())\n",
    "\n",
    "#please provide the name of the file that contains your data \n",
    "#(the file should be placed under the 'resources' folder)\n",
    "DATA_FILE_NAME='data.json'\n",
    "\n",
    "def parse(path):\n",
    "    reviews = open(path, 'r')\n",
    "    data = []\n",
    "    for review in reviews:\n",
    "        data.append(json.loads(review))\n",
    "    return data\n",
    "\n",
    "# Connecting to cloudant\n",
    "server = couchdbkit.Server(config['CLOUDANT']['CLOUDANT_URL'])\n",
    "db = server.get_or_create_db(config['CLOUDANT']['CLOUDANT_DB'])\n",
    "\n",
    "# Uploading data to cloudant\n",
    "data_file_path = os.path.join(curdir,'..','resources',DATA_FILE_NAME)\n",
    "docs = parse(data_file_path)\n",
    "logger.debug(\"uploading(\" + str(len(docs)) + \")...\")\n",
    "\n",
    "for review in xrange(0, len(docs), 1000):\n",
    "    db.bulk_save(docs[review : review + NUMBER_OF_REVIEWS])\n",
    "\n",
    "logger.info(\"Cloudant upload finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating .csv file for use with Watson Knowledge Studio\n",
    "The data must be converted into a form that can be used to train the models for entity extraction through Watson Knowledge Studio. To do this, run the script below. \n",
    "\n",
    "NOTE: If you stored your data in a separate database other than Cloudant during step 2, please edit the database connections so that they point to your database where all your data is stored or recode that portion so that it points to wherever the data is located.\n",
    "\n",
    "The important portion is the output and the format of the output csv that is ingested by Watson Knowledge Studio is:\n",
    "\n",
    "\"key\",\"value\"\n",
    "\n",
    "\"TITLE OF DOCUMENT\",\"TEXT OF DOCUMENT\"\n",
    "\n",
    "...\n",
    "\n",
    "The .csv that is created at the end of the script must now be imported into Watson Knowledge Studio (WKS). Each row in the .csv will be treated as a separate document and will be organized in WKS. The data doesn't have to be in .csv format to be uploaded into WKS, one can also do it manually by uploading documents but we provide a tool to convert it into .csv to make it easier.\n",
    "\n",
    "The documents then need to be annotated with entities and relationships. Coreference is also done here. A strict guideline is very helpful when doing this.\n",
    "\n",
    "For guidelines and tips on Watson Knowledge Studio, reference /notebooks/WKS.md\n",
    "\n",
    "After the annotations are done and the model is trained, it needs to be exported into Alchemy Language using an API key. To do this, first get an Alchemy API key from bluemix and then reference the WKS documentation above to see how that is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cloudant\n",
    "import csv\n",
    "import logging\n",
    "import configparser\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "logger.debug(config.sections())\n",
    "\n",
    "#please provide the name for the .csv that will be uploaded \n",
    "#to WKS (the file will be written to the 'data/output/' folder)\n",
    "WKS_INPUT_FILE='wks_input.csv'\n",
    "\n",
    "OUTPUT_FILE = os.path.join(curdir,'..','data','output',WKS_INPUT_FILE)\n",
    "\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "\n",
    "# Connect to the server\n",
    "client.connect()\n",
    "\n",
    "# Load the database \n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "\n",
    "# Process results from the query and write to a file\n",
    "try:\n",
    "    file = open(OUTPUT_FILE, 'wb')\n",
    "    writer = csv.writer(file)\n",
    "except:\n",
    "    logging.error('Error when opening file for writing.')\n",
    "\n",
    "for doc in db:\n",
    "    writer.writerow([doc['title'], doc['reviewText']])\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating groud truth file for NLC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A groud truth file (groud_truth.csv) is already provided for this example under the 'data' folder.\n",
    "\n",
    "If you are working on a different use case, create your own NLC groud truth data for training a new classifier by following the instructions and best practices available on the service tutorial (link available in the README).\n",
    "\n",
    "Once your groud_truth.csv file is created, save it to the 'data' folder so that the next steps can be run using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Using the WKS model to replace entities by their semantic types\n",
    "\n",
    "Once a WKS model is trained and hooked up to the alchemy API key, this information needs to be updated in your local .env file.\n",
    "\n",
    "This step allows Alchemy to replace instances mentioned in the sentences by their semantic types (color, product, model). These semantic types were defined when the WKS model was trained and are usually associated with a given domain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"root\"\n",
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:21: DeprecationWarning: You passed a bytestring as `filenames`. This will not work on Python 3. Use `cp.read_file()` or switch to using Unicode strings across the board.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'WKS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4bace141e6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WKS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WKS_MODEL_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m alchemy_api = alchemy.AlchemyLanguageV1(api_key = \n\u001b[1;32m     26\u001b[0m                     config['ALCHEMY']['ALCHEMY_API_KEY'])\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/backports/configparser/__init__.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_section\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'WKS'"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import csv\n",
    "from watson_developer_cloud import alchemy_language_v1 as alchemy\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "logger.debug(config.sections())\n",
    "\n",
    "model_id = config['WKS']['WKS_MODEL_ID']\n",
    "alchemy_api = alchemy.AlchemyLanguageV1(api_key = \n",
    "                    config['ALCHEMY']['ALCHEMY_API_KEY'])\n",
    "\n",
    "#please provide the path to the ground truth file you created on\n",
    "#step 4. The path provided by default points to the available ground\n",
    "#truth file\n",
    "INPUT_FILE = os.path.join(curdir,'..','data','ground_truth.csv')\n",
    "OUTPUT_FILE = os.path.join(curdir,'..','data',\n",
    "                           'output','ground_truth_replaced.csv')\n",
    "\n",
    "def get_entities(review):\n",
    "    split = {}\n",
    "    if len(review) > 5024:\n",
    "        mid = find_middle(review)\n",
    "        while mid >= 5024:\n",
    "            mid = find_middle(review[:mid])\n",
    "        review = review[:mid]\n",
    "        half = review[mid:]\n",
    "        split = get_entities(half)\n",
    "    f = alchemy_api.entities(text=review, model='', sentiment=True)\n",
    "    response = f.content\n",
    "    response = ast.literal_eval(response)\n",
    "    if split != {}:\n",
    "        if 'entities' in split and 'entities' in response:\n",
    "            response['entities'] = response['entities'] + split['entities']\n",
    "            response['text'] = response['text'] + split['text']\n",
    "        elif 'entites' in split and 'entities' not in response:\n",
    "            response['entities'] = split['entities']\n",
    "    return response\n",
    "\n",
    "\n",
    "def token_replacement_entities(review):\n",
    "    processed = get_entities(review)\n",
    "    if 'statusInfo' in processed:\n",
    "        return review\n",
    "    if 'entities' in processed:\n",
    "        entities = processed['entities']\n",
    "        text = processed['text']\n",
    "        for i in entities:\n",
    "            token = i['text']\n",
    "            classification = \"<\" + i['type'] + \">\"\n",
    "            token = re.escape(token)\n",
    "            re.sub(r'\\\\ ', ' ', token)\n",
    "            text = re.sub(r\"\\b%s\\b\" % token, classification, text, count=1)\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_middle(text):\n",
    "        generator = nltk.tokenize.util.regexp_span_tokenize(text, r'\\.')\n",
    "        sequences = list(generator)\n",
    "        mid_sentence = len(sequences)/2\n",
    "        middle_char = sequences[mid_sentence][1]\n",
    "        middle_char = int(middle_char) + 1\n",
    "        return middle_char\n",
    "    \n",
    "#Opens a file to write results from token replacement\n",
    "try:\n",
    "    write = open(OUTPUT_FILE, 'wb')\n",
    "    writer = csv.writer(write)\n",
    "except:\n",
    "    logging.error('Error when opening file for writing.')\n",
    "\n",
    "#Opens a file to read reviews\n",
    "try:\n",
    "    read = open(INPUT_FILE, 'rb') \n",
    "    reader = csv.reader(read)\n",
    "except:\n",
    "    logging.error('Error when opening file for reading.')\n",
    "\n",
    "for row in reader:\n",
    "    token = token_replacement_entities(row[0])\n",
    "    writer.writerow([token, row[1], row[2]])\n",
    "\n",
    "read.close()\n",
    "write.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training Natural Language Classifier\n",
    "\n",
    "The Natural Language Classifier (NLC) should be trained on the generalized/tagged data generated by the previous step.\n",
    "\n",
    "This step assumes that you have provided your NLC credentials in your local .env file.\n",
    "\n",
    "IMPORTANT: Update the local .env file to contain the following line in the [NLC] section:\n",
    "    NLC_CLASSIFIER = YOUR_CLASSIFIER_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:/Users/priscillamoraes/git/product-intelligence/notebooks\n",
      "DEBUG:root:[u'CLOUDANT', u'NLC', u'ALCHEMY', u'WKS']\n",
      "DEBUG:root:Classifier training initialized...\n",
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): gateway.watsonplatform.net\n",
      "DEBUG:requests.packages.urllib3.connectionpool:\"POST /natural-language-classifier/api/v1/classifiers HTTP/1.1\" 200 None\n",
      "DEBUG:root:Classifier training finished.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import csv\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "logger.debug(config.sections())\n",
    "\n",
    "NLC_USERNAME = config['NLC']['NLC_USERNAME']\n",
    "NLC_PASSWORD = config['NLC']['NLC_PASSWORD']\n",
    "\n",
    "#please provide the path to the ground truth file generated\n",
    "#on the previous step. The path provided by default points \n",
    "#to the available ground truth file example.\n",
    "TRAINING_DATA = os.path.join(curdir,'..','data','output',\n",
    "                           'ground_truth_replaced.csv')\n",
    "\n",
    "#initializing classifier\n",
    "nlc = NaturalLanguageClassifierV1(username=NLC_USERNAME, \n",
    "                                  password=NLC_PASSWORD)\n",
    "\n",
    "#training classifier\n",
    "logger.debug('Classifier training initialized...')\n",
    "with open(TRAINING_DATA, 'rb') as training_data:\n",
    "    classifier = nlc.create(\n",
    "        training_data=training_data,\n",
    "        name='voc_classifier_tokenized',\n",
    "        language='en'\n",
    "      )\n",
    "logger.debug('Classifier training finished.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting\n",
    "\n",
    "Some things to keep in mind while evaluating the results of the previous steps in your application:\n",
    "\n",
    "1. Increase the size of the training set for the NLC. This generally increases the performance but with diminishing returns as the size is increased.\n",
    "2. Balance the training/testing sets. What this means is that each of the classes needs to have comparable levels of representation in the training set so that the classifiers can learn about each of the classes and not overclassify for the larger classes.\n",
    "3. Skip entity replacement. Entity replacement is used as a way to generalize the data going into the NLC. The effect this can have is that it can reduce the accuracy of the system if the dataset is already specific to a tight domain. If that is the case for the application then removing this step can increase accuracy.\n",
    "4. Check WKS documentation for troubleshooting steps for WKS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
