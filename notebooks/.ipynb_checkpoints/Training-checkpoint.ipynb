{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data\n",
    "The first step is to find a data set that needs to be analyzed. The data needs to be stored so that the rest of the flow can use it. It is recommended to use a database and store the data in JSON format but any way of saving the data is fine as long as it can be accessed throughout the flow. Examples include just storing it in a JSON file, SQL database, NoSQL database, CSV file, etc. For the purposes of this starter kit, the data is being stored in a Cloudant NoSQL database (available on bluemix) and that is the simplest way to get started and requires the fewest changes to be made to the scripts. We offer a few scripts to help push and pull from this database to make it even easier.\n",
    "\n",
    "The minimum amount of data needed is review/feedback text and a way to link it to the product it is targeted to (most likely with some sort of key/id to mark a review for a product).\n",
    "\n",
    "Take the dataset and push it to your database/file in a format that is easiest to index and use further in the flow. For reference, here is an example of a review that we used: \n",
    "\n",
    "TODO: ADD JSON STRING HERE\n",
    "\n",
    "Key things to note: By storing it this way, it is easy to retrieve the text of the review and find out which product it is pointed to so that the rest of the flow can be easily executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data CSV into Watson Knowledge Studio\n",
    "The data must be converted into a form that can be used to train the models for entity extraction through Watson Knowledge Studio. To do this, run the script below. Please edit the database connections so that they point to your database where all your data is stored or recode that portion so that it points to wherever the data is located (if not in a cloudant database).\n",
    "\n",
    "The important portion is the output and the format of the output csv that is ingested by Watson Knowledge Studio is:\n",
    "\n",
    "\"key\",\"value\"\n",
    "\n",
    "\"TITLE OF DOCUMENT\",\"TEXT OF DOCUMENT\"\n",
    "\n",
    "...\n",
    "\n",
    "The CSV that is created at the end of the script must now be imported into Watson Knowledge Studio (WKS). Each row in the CSV will be treated as a separate document and will be organized in WKS. The data doesn't have to be in CSV format to be uploaded into WKS, one can also do it manually by uploading documents but we provide a tool to convert it into CSV to make it easier.\n",
    "\n",
    "The documents then need to be annotated with entities and relationships. Coreference is also done here. A strict guideline is very helpful when doing this.\n",
    "\n",
    "For guidelines and tips on Watson Knowledge Studio, reference /notebooks/WKS.md\n",
    "\n",
    "After the annotations are done and the model is trained, it needs to be exported into Alchemy Language using an API key. To do this, first get an Alchemy API key from bluemix and then reference the WKS documentation above to see how that is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#db2file.py\n",
    "\n",
    "import cloudant\n",
    "import csv\n",
    "\n",
    "SERVER = ''      ''' Replace with your server URL'''\n",
    "DATABASE = ''    ''' Replace with the name of the database'''\n",
    "USERNAME = ''    ''' Replace with the username from your\n",
    "                    credentials for the NLC'''\n",
    "PASSWORD = ''    ''' Replace with the password from\n",
    "                    your credentials for the NLC'''\n",
    "DESIGN = ''      ''' Replace with the name of the design document that contains\n",
    "                     the view. This should be of the form '_design/XXXX''''\n",
    "VIEW = ''        ''' Replace with the view from your database to poll,\n",
    "                    this should take the form of view_file/view and should\n",
    "                    return the text to classify as the value field and what\n",
    "                    you would like to call it as the key'''\n",
    "DESTINATION = ''  ''' Replace with correct name for output\n",
    "                    file (NOTE must be *.csv)'''\n",
    "\n",
    "server = cloudant.client.Cloudant(USERNAME, PASSWORD, url=SERVER)\n",
    "server.connect()\n",
    "db = server[DATABASE]\n",
    "query = db.get_view_result(DESIGN, VIEW)\n",
    "file = open(DESTINATION, 'wb')\n",
    "writer = csv.writer(file)\n",
    "\n",
    "\n",
    "for q in query:\n",
    "    print q[0]\n",
    "    if 'key' in q[0] and q[0]['key'] is not None:\n",
    "        title = q[0]['key']\n",
    "    else:\n",
    "        title = \"No Title\"\n",
    "    if 'value' in q[0] and q[0]['value'] is not None:\n",
    "        text = q[0]['value']\n",
    "    else:\n",
    "        text = \"No Text\"\n",
    "    writer.writerow([title, text])\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Alchemy API key and Model ID\n",
    "\n",
    "Once a WKS model is trained and hooked up to the alchemy API key. The info needs to be updated in the utils/token_replacement.py script. Just update the 2 variables at the top and that will get your script ready to use for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# token_replacement.py\n",
    "\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "from watson_developer_cloud import alchemy_language_v1 as alchemy\n",
    "\n",
    "apikey = ''  # Replace with your Alchemy Language API key\n",
    "modelId = ''  # Replace with the model-id from Watson Knowledge Studio\n",
    "alchemyapi = alchemy.AlchemyLanguageV1(api_key=apikey)\n",
    "\n",
    "\n",
    "def get_relations(review):\n",
    "    split = {}\n",
    "    if len(review) > 5024:\n",
    "        mid = find_middle(review)\n",
    "        while mid >= 5024:\n",
    "            mid = find_middle(review[:mid])\n",
    "        half = review[mid:]\n",
    "        review = review[:mid]\n",
    "        split = get_relations(half)\n",
    "    f = alchemyapi.typed_relations(text=review, model=modelId)\n",
    "    response = f.content\n",
    "    response = ast.literal_eval(response)\n",
    "\n",
    "    while response['status'] == 'ERROR':\n",
    "        if 'language' in response:\n",
    "            if response['language'] != 'english':\n",
    "                break\n",
    "        print response\n",
    "\n",
    "    if split != {}:\n",
    "        if 'typedRelations' in response and 'typedRelations' in split:\n",
    "            response['typedRelations'] = response['typedRelations'] + \\\n",
    "                split['typedRelations']\n",
    "            response['text'] = response['text'] + split['text']\n",
    "        elif 'typedRelations' in split and 'typedRelations' not in response:\n",
    "            response['typedRelations'] = split['typedRelations']\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_entities(review):\n",
    "    split = {}\n",
    "    if len(review) > 5024:\n",
    "        mid = find_middle(review)\n",
    "        while mid >= 5024:\n",
    "            mid = find_middle(review[:mid])\n",
    "        review = review[:mid]\n",
    "        half = review[mid:]\n",
    "        split = get_entities(half)\n",
    "    f = alchemyapi.entities(text=review, model=modelId, sentiment=True)\n",
    "    response = f.content\n",
    "    response = ast.literal_eval(response)\n",
    "    if split != {}:\n",
    "        if 'entities' in split and 'entities' in response:\n",
    "            response['entities'] = response['entities'] + split['entities']\n",
    "            response['text'] = response['text'] + split['text']\n",
    "        elif 'entites' in split and 'entities' not in response:\n",
    "            response['entities'] = split['entities']\n",
    "    return response\n",
    "\n",
    "\n",
    "def token_replacement_entities(review):\n",
    "    processed = get_entities(review)\n",
    "    if 'statusInfo' in processed:\n",
    "        return review\n",
    "    if 'entities' in processed:\n",
    "        entities = processed['entities']\n",
    "        text = processed['text']\n",
    "        for i in entities:\n",
    "            token = i['text']\n",
    "            classification = \"<\" + i['type'] + \">\"\n",
    "            token = re.escape(token)\n",
    "            re.sub(r'\\\\ ', ' ', token)\n",
    "            text = re.sub(r\"\\b%s\\b\" % token, classification, text, count=1)\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_middle(text):\n",
    "        generator = nltk.tokenize.util.regexp_span_tokenize(text, r'\\.')\n",
    "        sequences = list(generator)\n",
    "        mid_sentence = len(sequences)/2\n",
    "        middle_char = sequences[mid_sentence][1]\n",
    "        middle_char = int(middle_char) + 1\n",
    "        return middle_char\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Natural Language Classifier\n",
    "After the Watson Knowledge Studio language model is finished, a NLC needs to be created. Go to bluemix and create an instance of a NLC. Then a design for the classes and layers need to be made. These final classes are the endpoints of the system that are \"grouped\" together for a final processing step. These should be groups of sentences that you would be interested in. For example, when doing Amazon reviews, it is interesting to know what features a product has so sentences that are related to features are grouped into a class. Sentences that are related to customer service or pricing would be another class since that is also a point of interest in the dataset.\n",
    "\n",
    "If data classes are hierarchical (one class is a subclass of another), it may be useful to use a layered approach. We have used 2 layers of classifiers for our application, where output of one class goes into the next layer. We found that using a layered approach gives us better results that using multiple classes in a single layer. This is because, using a layered approach helps us eliminate miscellaneous sentences at each layer and pass more specific data to the next layer.  We have included our sample training sets in /src/Training/\"new training set\"/. Description of our layered architecture and classes can be found /notebooks/NLC_description.docx. You can define your own architecture, and you can experiment with a single layer if the classes are distinct. \n",
    "\n",
    "Note: We found it helpful to always include an \"Other\" category in the NLC to reject sentences that aren't useful to your application. Basically this acts as a junk bin.\n",
    "\n",
    "A script called src/utils/classify.py needs to be changed depending on what your final classification structure looks like. What this script does is take in an input sentence and runs it through the whole classifier tree and gives you the output from the whole tree. It also has a getClasses function should return all the endpoint classes in your system for use in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify.py\n",
    "\n",
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "\n",
    "CLF_USERNAME = ''  # Replace with the username from your credentials for NLC\n",
    "CLF_PASSWORD = ''  # Replace with the password from your credentials for NLC\n",
    "CLASSIFIER_JSON = '../../data/classifier_ids.json'  # Location of classifiers that is autogenerated when you train the NLC\n",
    "\n",
    "# Retrieve Classifier ID's\n",
    "with open(CLASSIFIER_JSON) as classifier_ids:\n",
    "    classifierTree = json.load(classifier_ids)\n",
    "\n",
    "nlc = NaturalLanguageClassifierV1(username=CLF_USERNAME, password=CLF_PASSWORD)\n",
    "\n",
    "# TODO Make general, do not bind to our NLC classes\n",
    "\n",
    "def classify(review):\n",
    "    # classifierTree holds all of the layers and they are referenced by tierX where X is the layer number\n",
    "    resp = nlc.classify(classifierTree['tier1'], review)\n",
    "    classification = resp[\"top_class\"]\n",
    "    # If an output of tier1 is to be fed into tier2, this is how it is done\n",
    "    if(classification == \"Product\"):\n",
    "        resp = nlc.classify(classifierTree['tier2'], review)\n",
    "        classification = resp[\"top_class\"]\n",
    "        # this is an example of feeding another output into tier3\n",
    "        if(classification == \"Feature\"):\n",
    "            resp = nlc.classify(classifierTree['tier3'], review)\n",
    "            classification = resp[\"top_class\"]\n",
    "    return classification\n",
    "\n",
    "# Returns the final classifications possible from the classifier architecture\n",
    "def getClasses():\n",
    "    return [\"Comparison\", \"Sentiment\", \"Customer Service\", \"Other\", \"Price\",\n",
    "            \"Issue\", \"Enhancement\", \"Feature\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and testing sets\n",
    "In order to train and validate your Natural Language Classifier, a training and testing set must be created. Run the script below to make these sets. It will create your training and testing sets in a CSV format that can be ingested by the NLC later in the flow. This will split up reviews into sentences and then leave those sentences to be classified by hand with your NLC design. \n",
    "\n",
    "The format for hand classifying looks like:\n",
    "\n",
    "Sentence1, Class\n",
    "\n",
    "Sentence2, Class\n",
    "\n",
    "...\n",
    "\n",
    "If a layered architecture is being used for the NLC, then a little complexity is added to the process. A training set must be created for each of the layers. A simple way to approach a layered architecture is as follows:\n",
    "1. First run the script below to create a training set\n",
    "2. Classify for 1st layer only and save the csv\n",
    "3. Now any class that is fed into a second layer needs to reclassified using second layer classes and saved into another csv.\n",
    "4. Continue until all layers have their own training set.\n",
    "\n",
    "A testing set for a layered architecture is much simpler. Just classify the sentences using the final output classes of the whole classifier tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training_testing.py\n",
    "\n",
    "'''\n",
    "This script creates a training and testing split out of your data for you. It\n",
    "can either be run interactivly by running the script witout arguments, or run\n",
    "automatically with command line arguments.\n",
    "The flags are:\n",
    "    -l The location of the data, a directory of .txt or .json files or a .csv\n",
    "        file\n",
    "    -r Percentage of data to split into training\n",
    "    -e Precentage of data to split into testing\n",
    "    -j Field in .json that contains the text data. Only necessary if loading\n",
    "        from a .json file.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import getopt\n",
    "import nltk\n",
    "\n",
    "\n",
    "def csv_handler(file, training, testing):\n",
    "    ftest = open(\"testing_set.csv\", \"wb\")\n",
    "    ftrain = open(\"training_set.csv\", \"wb\")\n",
    "    wtest = csv.writer(ftest)\n",
    "    wtrain = csv.writer(ftrain)\n",
    "\n",
    "    rand = np.random.rand\n",
    "\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        if rand() < training:\n",
    "            sentences = nltk.tokenize.sent_tokenize(row)\n",
    "            for sentence in sentences:\n",
    "                wtrain.writerow([sentence])\n",
    "        else:\n",
    "            sentences = nltk.tokenize.sent_tokenize(row)\n",
    "            for sentence in sentences:\n",
    "                wtest.writerow([sentence])\n",
    "\n",
    "    ftest.close()\n",
    "    ftrain.close()\n",
    "\n",
    "\n",
    "def txt_handler(file, writer):\n",
    "\n",
    "    text = file.read()\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        writer.writerow([sentences])\n",
    "\n",
    "\n",
    "def json_handler(file, writer, json_field):\n",
    "    raw_text = file.read()\n",
    "\n",
    "    try:\n",
    "        processed_text = ast.literal_eval(raw_text)\n",
    "        text = processed_text[json_field]\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            writer.writerow([sentences])\n",
    "    except:\n",
    "        print \"ERROR: Something wrong with .json file: \" + file.name\n",
    "flags = {}\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    args = sys.argv[1:]\n",
    "    opts = getopt.getopt(args, 'l:e:r:j:')\n",
    "    for pair in opts:\n",
    "        flags[pair[0]] = pair[1]\n",
    "\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    print \"Input the full path to your data\"\n",
    "    print \"The data can be in the format of a .csv file (with one column\" + \\\n",
    "        \" and one text per line), or a directory of .json or .txt files.\"\n",
    "    print \"NOTE: If you are using a directory, please make sure your data\" + \\\n",
    "        \"is the only thing in the directory\"\n",
    "    location = raw_input(\"Data Location: \")\n",
    "else:\n",
    "    if '-l' in flags:\n",
    "        location = flags['-l']\n",
    "    else:\n",
    "        print \"ERROR: No file location. Did you use the -l\" +\\\n",
    "            \" flag to mark a file location?\"\n",
    "\n",
    "if re.match(r\".\\.csv$\", location):\n",
    "    try:\n",
    "        f = open(location, 'rb')\n",
    "        training = 0\n",
    "        testing = 0\n",
    "        if len(sys.argv) > 1:\n",
    "            while training + testing != 100:\n",
    "                print \"What fraction would you like to use for training?\" + \\\n",
    "                    \" (We recommend 70%)\"\n",
    "                training = raw_input(\"Training (0-100): \")\n",
    "                print \"What fraction would you like to use for testing?\" + \\\n",
    "                    \" (We recommend 30%)\"\n",
    "                testing = raw_input(\"Testing (0-100): \")\n",
    "                if training + testing != 100:\n",
    "                    print \"ERROR: Training and testing sets must equal 100%\"\n",
    "        else:\n",
    "            if '-r' in flags and '-e' in flags:\n",
    "                training = flags['-r']\n",
    "                testing = flags['-e']\n",
    "            else:\n",
    "                print \"ERROR: No training or testing split.\" + \\\n",
    "                    \" Did you use the -r and -e flags to mark them?\"\n",
    "\n",
    "        training = float(training)/100\n",
    "        testing = float(testing)/100\n",
    "\n",
    "        csv_handler(f, training, testing)\n",
    "        f.close()\n",
    "\n",
    "    except (IOError):\n",
    "        print \"ERROR: File not found\"\n",
    "\n",
    "else:\n",
    "    json_field = \"\"\n",
    "    try:\n",
    "        files = os.listdir(location)\n",
    "        rand = np.random.rand\n",
    "        total_docs = len(files)\n",
    "        training = 0\n",
    "        testing = 0\n",
    "        if len(sys.argv) > 1:\n",
    "            while training + testing != 100:\n",
    "                print \"What fraction would you like to use for training?\" + \\\n",
    "                    \" (We recommend 70%)\"\n",
    "                training = input(\"Training (0-100): \")\n",
    "                print \"What fraction would you like to use for testing?\" + \\\n",
    "                    \" (We recommend 30%)\"\n",
    "                testing = input(\"Testing (0-100): \")\n",
    "                if training + testing != 100:\n",
    "                    print \"ERROR: Training and testing sets must equal 100%\"\n",
    "        else:\n",
    "            if '-r' in flags and '-e' in flags:\n",
    "                training = flags['-r']\n",
    "                testing = flags['-e']\n",
    "            else:\n",
    "                print \"ERROR: No training or testing split.\" + \\\n",
    "                    \" Did you use the -r and -e flags to mark them?\"\n",
    "\n",
    "        training = float(training)/100\n",
    "        testing = float(testing)/100\n",
    "\n",
    "        ftest = open(\"testing_set.csv\", \"wb\")\n",
    "        ftrain = open(\"training_set.csv\", \"wb\")\n",
    "        wtest = csv.writer(ftest)\n",
    "        wtrain = csv.writer(ftrain)\n",
    "\n",
    "        for entry in files:\n",
    "            if re.match(r\".\\.txt$\", entry):\n",
    "                f = open(location + '/' + entry, 'rb')\n",
    "\n",
    "                if rand() < training:\n",
    "                    txt_handler(f, wtrain)\n",
    "                else:\n",
    "                    txt_handler(f, wtest)\n",
    "                f.close()\n",
    "            if re.match(r\".\\.json$\", entry):\n",
    "                if len(sys.argv) > 1:\n",
    "                    if json_field == \"\":\n",
    "                        print \"What key in the .json contains your text data?\"\n",
    "                        json_field = raw_input(\"Json Key: \")\n",
    "                else:\n",
    "                    if '-j' in flags:\n",
    "                        json_fields = flags['-j']\n",
    "                    if json_field == \"\":\n",
    "                        print \"Please use the -j flag to give the key of\" + \\\n",
    "                            \"the .json that contains the text data.\"\n",
    "\n",
    "                f = open(location + '/' + entry, 'rb')\n",
    "\n",
    "                if rand() < training:\n",
    "                    json_handler(f, wtrain, json_field)\n",
    "                else:\n",
    "                    json_handler(f, wtest, json_field)\n",
    "                f.close()\n",
    "        ftest.close()\n",
    "        ftrain.close()\n",
    "    except OSError:\n",
    "        print \"ERROR: Directory not found\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Entity Extraction and Replacement\n",
    "The next thing that needs to be done is take the training and testing sets and replace any entities found in the sentences with a tag of the entity name found. This is where a sentence is generalized like so: \n",
    "\n",
    "I love my phone because of its great screen --> I love my Product because of its Descriptor Feature\n",
    "\n",
    "This allows the NLC to be generalized when training so that it works across many products. To perform this, run the script below (src/Training/Auto_Token_Replacement.py) and just replace the read and write files with the files you are trying to get entity replaced. So for a single layer NLC, first run it with the training set and then the testing set and give the output files easily identified names to let you know that the data has been replaced (something simple like \"training_replaced.csv\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Auto_Token_Replacement.py\n",
    "\n",
    "'''\n",
    "Takes a csv file with one record per line, uses the alchemy languag API to find\n",
    "keywords and then replaces the keywords of the record with the name of the\n",
    "class of the keywords. The end result is a generalized sentence.\n",
    "Must change location of input .csv and output .csv\n",
    "'''\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from utils import token_replacement as t\n",
    "\n",
    "read = open('', 'rb')      # Replace with location of .csv file to classify\n",
    "write = open('', 'wb')     # Replace with output file location\n",
    "\n",
    "reader = csv.reader(read)\n",
    "writer = csv.writer(write)\n",
    "\n",
    "for row in reader:\n",
    "    token = t.token_replacement_entities(row[0])\n",
    "    writer.writerow([token, row[1], row[2]])\n",
    "\n",
    "read.close()\n",
    "write.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Natural Language Classifier\n",
    "The Natural Language Classifier (NLC) needs to be trained on this generalized/tagged data. This is a 3 level NLC that will provide better classification results than just using a single layer. Add your credentials in the file below and run the script to train the NLC. The script will also store your classifier_id's in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainNLC.py\n",
    "\n",
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "\n",
    "tier1CSV = ''  # Replace with location of layer 1 training data\n",
    "tier2CSV = ''  # Replace with another location of layer 2 training data\n",
    "USERNAME = ''  # Replace with username of NLC credentials\n",
    "PASSWORD = ''  # Replace with password of NLC credentials\n",
    "JSON_TARGET = '../../data/classifier_ids.json'  # Location to keep classifiers\n",
    "\n",
    "# The architecture of your classifier tree is used here. This is 2 tiers but add/remove any tiers to make\n",
    "# it work with your architecture\n",
    "classifierTree = {\n",
    "    'tier1': '',\n",
    "    'tier2': ''\n",
    "}\n",
    "\n",
    "# Initialize classifier\n",
    "nlc = NaturalLanguageClassifierV1(username=USERNAME, password=PASSWORD)\n",
    "\n",
    "# Train tier 1 classifier\n",
    "print(\"############# TIER 1 CLASSIFIER ##############\")\n",
    "with open(tier1CSV, 'rb') as training_data:\n",
    "    classifier = nlc.create(\n",
    "        training_data=training_data,\n",
    "        name='tier1',\n",
    "        language='en'\n",
    "      )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier1'] = classifier['classifier_id']\n",
    "\n",
    "# Train tier 2 classifier\n",
    "print(\"############# TIER 2 CLASSIFIER ##############\")\n",
    "with open(tier2CSV, 'rb') as training_data:\n",
    "    classifier = nlc.create(\n",
    "        training_data=training_data,\n",
    "        name='tier2',\n",
    "        language='en'\n",
    "      )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier2'] = classifier['classifier_id']\n",
    "\n",
    "\n",
    "# Write the tiers with classifier id's to file for use later\n",
    "with open(JSON_TARGET, 'w') as outfile:\n",
    "    json.dump(classifierTree, outfile)\n",
    "\n",
    "print(\"############# FULL CLASSIFIER TREE ##############\")\n",
    "print(json.dumps(classifierTree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Results\n",
    "Now since all the models are now trained, they can be validated to make sure the performance being attained is good enough to use. Run the src/Training/ValidateModels.py script to see how the trained models performed. It will tell you your classification accuracy (number of times you got the class right divided by number of tests) and a confusion matrix that tells you HOW you are misclassifying things so that you can improve results even more.\n",
    "\n",
    "A score of around 60% is usually appropriate from testing but it is very dependent on the application and the amount of data that is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ValidateModels.py\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from utils import token_replacement as tr\n",
    "from utils import classify as clf\n",
    "\n",
    "TEST_SET_FILE = '' # Insert the name of your test set that has gone through entity replacement\n",
    "verbose = False\n",
    "\n",
    "read = open(TEST_SET_FILE, 'rb')\n",
    "reader = csv.reader(read)\n",
    "\n",
    "# confusion_matrix[PREDICTED][ACTUAL]\n",
    "confusion_matrix = {}\n",
    "classes = clf.getClasses()\n",
    "for cl1 in classes:\n",
    "    confusion_matrix[cl1] = {}\n",
    "    for cl2 in classes:\n",
    "        confusion_matrix[cl1][cl2] = 0\n",
    "\n",
    "num_correct = 0\n",
    "total = 0\n",
    "for row in reader:\n",
    "    sentence = row[0]\n",
    "    correct_class = row[1]\n",
    "    total += 1\n",
    "    correct = False\n",
    "    token = tr.token_replacement(sentence)\n",
    "    classification = clf.classify(token)\n",
    "    confusion_matrix[classification][correct_class] += 1\n",
    "    if(classification in correct_class):\n",
    "        num_correct += 1\n",
    "        correct = True\n",
    "    if(verbose):\n",
    "        print(sentence + \" --> \" + token)\n",
    "        print(\"EXPECTED: \" + correct_class + \" / ACTUAL: \" + classification)\n",
    "        if(correct):\n",
    "            print(\"CORRECT!\")\n",
    "        else:\n",
    "            print(\"WRONG!\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "print(\"CORRECT: \" + str(num_correct) + \"/\" + str(total) + \" (\" +\n",
    "      str(num_correct/float(total)*100) + \")\")\n",
    "\n",
    "row = ''\n",
    "for i in range(len(classes)+1):\n",
    "    row += '{data[' + str(i) + ']:>20} |'\n",
    "headers = [\"\"] + classes\n",
    "print(row.format(data=headers))\n",
    "\n",
    "for cl1 in classes:\n",
    "    row_data = []\n",
    "    row_data.append(cl1)\n",
    "    for cl2 in classes:\n",
    "        row_data.append(confusion_matrix[cl1][cl2])\n",
    "    print(row.format(data=row_data))\n",
    "\n",
    "read.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting\n",
    "If the results from validation are not good, there are a few steps that can help boost those results to an acceptable level.\n",
    "\n",
    "1. Increase the size of the training set for the NLC. This generally increases the performance but with diminishing returns as the size is increased.\n",
    "2. Balance the training/testing sets. What this means is that each of the classes needs to have comparable levels of representation in the training set so that the classifiers can learn about each of the classes and not overclassify for the larger classes.\n",
    "3. Check the confusion matrix and see which classes are being misclassified the most and focus on giving them more data. If some classes are overclassified then look at tip #2.\n",
    "4. Skip entity replacement. Entity replacement is used as a way to generalize the data going into the NLC. The effect this can have is that it can reduce the accuracy of the system if the dataset is already specific to a tight domain. If that is the case for the application then removing this step can increase accuracy.\n",
    "5. Check WKS documentation for troubleshooting steps for WKS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
