{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data\n",
    "The first step is to find a data set that needs to be analyzed. The data needs to be stored so that the rest of the flow can use it. It is recommended to use a database and store the data in JSON format but any way of saving the data is fine as long as it can be accessed throughout the flow. Examples include just storing it in a JSON file, SQL database, NoSQL database, CSV file, etc. For the purposes of this starter kit, the data is being stored in a Cloudant NoSQL database (available on bluemix) and that is the simplest way to get started and requires the fewest changes to be made to the scripts. We offer a few scripts to help push and pull from this database to make it even easier.\n",
    "\n",
    "The minimum amount of data needed is review/feedback text and a way to link it to the product it is targeted to (most likely with some sort of key/id to mark a review for a product).\n",
    "\n",
    "Take the dataset and push it to your database/file in a format that is easiest to index and use further in the flow. For reference, here is an example of a review that we used: \n",
    "\n",
    "TODO: ADD JSON STRING HERE\n",
    "\n",
    "Key things to note: By storing it this way, it is easy to retrieve the text of the review and find out which product it is pointed to so that the rest of the flow can be easily executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Data into CSV\n",
    "The data must be converted into a form that can be used to train the models for entity extraction. To do this, run the script below. Please edit the database connections so that they point to your database where all your data is stored or recode that portion so that it points to wherever the data is located (if not in a cloudant database).\n",
    "\n",
    "The important portion is the output and the format of the output csv that is ingested by Watson Knowledge Studio is:\n",
    "\n",
    "\"key\",\"value\"\n",
    "\"TITLE OF DOCUMENT\",\"TEXT OF DOCUMENT\"\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data CSV into Watson Knowledge Studio\n",
    "The data must be converted into a form that can be used to train the models for entity extraction through Watson Knowledge Studio. To do this, run the script below. Please edit the database connections so that they point to your database where all your data is stored or recode that portion so that it points to wherever the data is located (if not in a cloudant database).\n",
    "\n",
    "The important portion is the output and the format of the output csv that is ingested by Watson Knowledge Studio is:\n",
    "\n",
    "\"key\",\"value\"\n",
    "\"TITLE OF DOCUMENT\",\"TEXT OF DOCUMENT\"\n",
    "...\n",
    "\n",
    "The CSV that is created at the end of the script must now be imported into Watson Knowledge Studio (WKS). Each row in the CSV will be treated as a separate document and will be organized in WKS. The data doesn't have to be in CSV format to be uploaded into WKS, one can also do it manually by uploading documents but we provide a tool to convert it into CSV to make it easier.\n",
    "\n",
    "The documents then need to be annotated with entities and relationships. Coreference is also done here. A strict guideline is very helpful when doing this.\n",
    "\n",
    "For guidelines and tips on Watson Knowledge Studio, reference /notebooks/WKS.md\n",
    "\n",
    "After the annotations are done and the model is trained, it needs to be exported into Alchemy Language using an API key. To do this, first get an Alchemy API key from bluemix and then reference the WKS documentation above to see how that is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cloudant\n",
    "import csv\n",
    "\n",
    "SERVER = ''      ''' Replace with your server URL'''\n",
    "DATABASE = ''    ''' Replace with the name of the database'''\n",
    "USERNAME = ''    ''' Replace with the username from your\n",
    "                    credentials for the NLC'''\n",
    "PASSWORD = ''    ''' Replace with the password from\n",
    "                    your credentials for the NLC'''\n",
    "DESIGN = ''      ''' Replace with the name of the design document that contains\n",
    "                     the view. This should be of the form '_design/XXXX''''\n",
    "VIEW = ''        ''' Replace with the view from your database to poll,\n",
    "                    this should take the form of view_file/view and should\n",
    "                    return the text to classify as the value field and what\n",
    "                    you would like to call it as the key'''\n",
    "DESTINATION = ''  ''' Replace with correct name for output\n",
    "                    file (NOTE must be *.csv)'''\n",
    "\n",
    "server = cloudant.client.Cloudant(USERNAME, PASSWORD, url=SERVER)\n",
    "server.connect()\n",
    "db = server[DATABASE]\n",
    "query = db.get_view_result(DESIGN, VIEW)\n",
    "file = open(DESTINATION, 'wb')\n",
    "writer = csv.writer(file)\n",
    "\n",
    "\n",
    "for q in query:\n",
    "    print q[0]\n",
    "    if 'key' in q[0] and q[0]['key'] is not None:\n",
    "        title = q[0]['key']\n",
    "    else:\n",
    "        title = \"No Title\"\n",
    "    if 'value' in q[0] and q[0]['value'] is not None:\n",
    "        text = q[0]['value']\n",
    "    else:\n",
    "        text = \"No Text\"\n",
    "    writer.writerow([title, text])\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace entities with Tags in Training Sets\n",
    "You will need to run your training set CSV through the script below which replaces any entities found by Entity Extraction in Alchemy language with a representative tag in order generalize the classifier in the coming steps. Just identify the name for your csv file input and output and it should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import token_replacement as t\n",
    "import csv\n",
    "\n",
    "read = open('ground_truth_layer1.csv','rb') ##replace with correct filenames\n",
    "write = open('ground_truth_layer1_replace.csv','wb')     ##replace with correct filenames\n",
    "\n",
    "reader = csv.reader(read)\n",
    "writer = csv.writer(write)\n",
    "\n",
    "for row in reader:\n",
    "    token = t.token_replacement(row[0])\n",
    "    writer.writerow([token,row[1]])\n",
    "\n",
    "read.close()\n",
    "write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Natural Language Classifier\n",
    "The Natural Language Classifier (NLC) needs to be trained on this generalized/tagged data. This is a 3 level NLC that will provide better classification results than just using a single layer. Add your credentials in the file below and run the script to train the NLC. The script will also store your classifier_id's in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import couchdbkit\n",
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "import csv\n",
    "\n",
    "tier1CSV = 'training_set1.csv'\n",
    "tier2CSV = 'training_set2.csv'\n",
    "tier3CSV = 'training_set3.csv'\n",
    "USERNAME = 'e561bc30-d294-41f4-8b47-39fc6bc29917'\n",
    "PASSWORD = 'XH8pYnsYfClv'\n",
    "JSON_TARGET = '../../data/classifier_ids.json'\n",
    "\n",
    "classifierTree = {\n",
    "    'tier1':'',\n",
    "    'tier2':'',\n",
    "    'tier3':''\n",
    "}\n",
    "\n",
    "# Initialize classifier\n",
    "nlc = NaturalLanguageClassifierV1(username = USERNAME, password = PASSWORD)\n",
    "\n",
    "# Train tier 1 classifier\n",
    "print(\"############# TIER 1 CLASSIFIER ##############\")\n",
    "with open(tier1CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier1',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier1'] = classifier['classifier_id']\n",
    "\n",
    "# Train tier 2 classifier\n",
    "print(\"############# TIER 2 CLASSIFIER ##############\")\n",
    "with open(tier2CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier2',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier2'] = classifier['classifier_id']\n",
    "\n",
    "# Train tier 3 classifier\n",
    "print(\"############# TIER 3 CLASSIFIER ##############\")\n",
    "with open(tier3CSV, 'rb') as training_data:\n",
    "  classifier = nlc.create(\n",
    "    training_data=training_data,\n",
    "    name='tier3',\n",
    "    language='en'\n",
    "  )\n",
    "print(json.dumps(classifier, indent=2))\n",
    "classifierTree['tier3'] = classifier['classifier_id']\n",
    "\n",
    "# Write the tiers with classifier id's to file for use later\n",
    "with open(JSON_TARGET, 'w') as outfile:\n",
    "    json.dump(classifierTree, outfile)\n",
    "\n",
    "print(\"############# FULL CLASSIFIER TREE ##############\")\n",
    "print(json.dumps(classifierTree))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
