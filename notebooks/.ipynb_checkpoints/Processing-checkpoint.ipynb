{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Flow\n",
    "\n",
    "The processing flow is run on all of the review data once the models are all trained up and validated. This is the flow that will actually turn the raw data into processed output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Token Replacement\n",
    "\n",
    "This step allows Alchemy to replace words in the sentences of the reviews by their semantic types (product, customer_service, company, etc). These semantic types were defined when the WKS model was trained and are usually associated with a given domain data.\n",
    "\n",
    "It replaces tokens from the 'reviewText' field of the documents stored in the database. It saves the replaced sentences to the 'taggedRevie' field in the same document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import cloudant\n",
    "from watson_developer_cloud import AlchemyLanguageV1\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "logger.debug(config.sections())\n",
    "\n",
    "model_id = config['WKS']['WKS_MODEL_ID']\n",
    "alchemy_api = AlchemyLanguageV1(api_key = \n",
    "                    config['ALCHEMY']['ALCHEMY_API_KEY'])\n",
    "\n",
    "\n",
    "def get_entities(review):\n",
    "    #logger.debug(review)\n",
    "    response = ''\n",
    "    try:\n",
    "        response = alchemy_api.entities(text=str(review), model=model_id)\n",
    "    except:\n",
    "        logger.error(\"Error when getting entities.\")\n",
    "    logger.debug(\"Result from entities call: \"+str(response))\n",
    "    return response\n",
    "\n",
    "def get_sentiment(review):\n",
    "    response = alchemy_api.sentiment(text=str(review))\n",
    "    logger.debug(\"Result from sentiment call: \"+str(response))\n",
    "    if 'docSentiment' in response:\n",
    "        if 'type' in response['docSentiment']:\n",
    "            return response['docSentiment']['type']\n",
    "        else:\n",
    "            return \" \"\n",
    "    else:\n",
    "        return \" \"\n",
    "\n",
    "def token_replacement_entities(review):\n",
    "    entities_response = get_entities(review)\n",
    "    features = []\n",
    "    if 'entities' in entities_response:\n",
    "        if len(entities_response['entities']) == 0:\n",
    "            logger.debug(\"ZERO entities found, returning review: \"+str(review))\n",
    "            return review, features\n",
    "        else:\n",
    "            entities = entities_response['entities']\n",
    "            logger.debug(\"List of entities found: \"+str(entities))\n",
    "            for i in entities:\n",
    "                token = i['text']\n",
    "                if ('Feature' in i['type']):\n",
    "                    features.append(token)\n",
    "                classification = \"<\" + i['type'] + \">\"\n",
    "                token = re.escape(token)\n",
    "                re.sub(r'\\\\ ', ' ', token)\n",
    "                review = re.sub(r\"\\b%s\\b\" % token, classification, review, count=1)\n",
    "            return review, features\n",
    "    else:\n",
    "        return review, features\n",
    "    \n",
    "#Initializing Cloudant client\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "\n",
    "#Going through all the documents and replacing the tokens by\n",
    "#their semantic types. Result is save back to the Cloudant document\n",
    "#in the 'taggedReview' field.\n",
    "client.connect()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "counter = 0\n",
    "for doc in db:\n",
    "    counter +=1\n",
    "    #logger.debug(doc)\n",
    "    try:\n",
    "        doc['taggedReview'], doc['features'] = \\\n",
    "            token_replacement_entities(doc['reviewText'])\n",
    "        doc.save()\n",
    "    except:\n",
    "        logger.error('Error saving tagged review to Cloudant document.')\n",
    "    try:\n",
    "        doc['sentiment'] = get_sentiment(doc['reviewText'])\n",
    "        doc.save()\n",
    "        logger.debug(doc['sentiment'])\n",
    "    except:\n",
    "        logger.error('Error saving sentiment to Cloudant document.')\n",
    "    #if (counter == 2):\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification of reviews\n",
    "\n",
    "This step uses the Natural Language Classifier (NLC) created on the Training notebook. This step classifies a review and adds the result of the classification to a new field (called 'class') of the document in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import cloudant\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "\n",
    "NLC_USERNAME = config['NLC']['NLC_USERNAME']\n",
    "NLC_PASSWORD = config['NLC']['NLC_PASSWORD']\n",
    "NLC_CLASSIFIER = config['NLC']['NLC_CLASSIFIER']\n",
    "\n",
    "#initializing classifier object\n",
    "nlc = NaturalLanguageClassifierV1(username=NLC_USERNAME, \n",
    "                                  password=NLC_PASSWORD)\n",
    "\n",
    "def classify(review):\n",
    "    logger.debug(review)\n",
    "    #Classify sentence\n",
    "    try:\n",
    "        response = nlc.classify(NLC_CLASSIFIER, review)\n",
    "        logger.debug(response)\n",
    "        if len(response['classes']) > 1:\n",
    "            return response['classes']\n",
    "    except:\n",
    "        logger.error('Failed at sentence classification')\n",
    "        return 'no class'\n",
    "\n",
    "#Initializing Cloudant client\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "\n",
    "client.connect()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "for doc in db:\n",
    "    logger.debug(doc)\n",
    "    try:\n",
    "        doc['class'] = classify(doc['taggedReview'])\n",
    "        doc.save()\n",
    "    except:\n",
    "        logger.error('Error saving classification to Cloudant document.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grouping products\n",
    "\n",
    "This script will create a product document in the database for each product that has been reviewed. It will also attach a list of the review id's to the new product document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): 204f49bc-b226-413d-8dcf-aece9c16ce89-bluemix.cloudant.com\n"
     ]
    }
   ],
   "source": [
    "import cloudant\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import couchdb\n",
    "import json\n",
    "import ast\n",
    "from sets import Set\n",
    "from flask import jsonify\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "client.connect()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "\n",
    "#Using couchdb instead of Cloudant\n",
    "couch = couchdb.Server(\"https://%s.cloudant.com\" % config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "couch.resource.credentials = (config['CLOUDANT']['CLOUDANT_USERNAME'], config['CLOUDANT']['CLOUDANT_PASSWORD'])\n",
    "\n",
    "if couch['products'] is not None:\n",
    "    couch.delete('products')\n",
    "db_output = couch.create('products')\n",
    "\n",
    "#Creating a dictionary to store the metadata about each product\n",
    "products = {}\n",
    "allFeatures = Set()\n",
    "for doc in db:\n",
    "    if doc['title']:\n",
    "        if doc['title'] not in products:\n",
    "            products[doc['title']] = {}\n",
    "            products[doc['title']]['product_name'] = doc['title']\n",
    "            products[doc['title']]['customer_service'] = {'sentiment': {}}\n",
    "            products[doc['title']]['customer_service']['sentiment']['posCount'] = 0\n",
    "            products[doc['title']]['customer_service']['sentiment']['neuCount'] = 0\n",
    "            products[doc['title']]['customer_service']['sentiment']['negCount'] = 0\n",
    "            products[doc['title']]['features'] = doc['features']\n",
    "            products[doc['title']]['issues'] = {'percentage': 0,'review_ids': []}\n",
    "            products[doc['title']]['reviewCount'] = 0\n",
    "            products[doc['title']]['product_id'] = doc['asin']\n",
    "    \n",
    "        products[doc['title']]['reviewCount'] += 1\n",
    "        #add logic to check for issue above a certain threshold and update the count and review id\n",
    "        if 'class' in doc:\n",
    "            classes = doc['class']\n",
    "            for classification in classes:\n",
    "                if (\"class_name\" in classification):\n",
    "                    classification = dict(classification)\n",
    "                    if (classification['class_name'] == \"Issue\") and (classification['confidence'] > 0.1):\n",
    "                        products[doc['title']]['issues']['review_ids'].append(doc['_id'])\n",
    "        #updating the sentiment count\n",
    "        if 'sentiment' in doc:\n",
    "            if doc['sentiment'] == 'positive':\n",
    "                products[doc['title']]['customer_service']['sentiment']['posCount'] += 1\n",
    "            elif doc['sentiment'] == 'neutral':\n",
    "                products[doc['title']]['customer_service']['sentiment']['neuCount'] += 1\n",
    "            elif doc['sentiment'] == 'negative':\n",
    "                products[doc['title']]['customer_service']['sentiment']['negCount'] += 1\n",
    "        #updating the list of features\n",
    "        if 'features' in doc:\n",
    "            classes = doc['class']\n",
    "            for classification in classes:\n",
    "                if (\"class_name\" in classification):\n",
    "                    classification = dict(classification)\n",
    "                    if (classification['class_name'] == \"Feature\") and (classification['confidence'] > 0.3):\n",
    "                        for feature in doc['features']:\n",
    "                            allFeatures.add(feature)\n",
    "                        for feature in products[doc['title']]['features']:\n",
    "                            allFeatures.add(feature)\n",
    "                        products[doc['title']]['features'] = list(allFeatures)\n",
    "                        allFeatures.clear()\n",
    "    \n",
    "#Pushing products to Cloudant database\n",
    "for product, value in products.items():\n",
    "    value['issues']['percentage'] = round(float(len(value['issues']['review_ids'])) /\\\n",
    "                                          value['reviewCount'], 3)\n",
    "    db_output.save(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering product feature sentences\n",
    "\n",
    "The script that runs the clustering on the review data is src/Processing/clustering.py\n",
    "A word2vec model is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): 204f49bc-b226-413d-8dcf-aece9c16ce89-bluemix.cloudant.com\n",
      "INFO:gensim.models.word2vec:loading projection weights from /Users/priscillamoraes/git/product-intelligence/notebooks/../data/sample_model.bin\n",
      "INFO:gensim.models.word2vec:loaded (71290, 200) matrix from /Users/priscillamoraes/git/product-intelligence/notebooks/../data/sample_model.bin\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import cloudant\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "\n",
    "client = cloudant.client.Cloudant(config['CLOUDANT']['CLOUDANT_USERNAME'],\n",
    "                                  config['CLOUDANT']['CLOUDANT_PASSWORD'],\n",
    "                                  account=config['CLOUDANT']['CLOUDANT_USERNAME'])\n",
    "client.connect()\n",
    "products_db = client['products']\n",
    "reviews_db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "\n",
    "#Please provide the path to the word2vec model you created. \n",
    "#The path provided by default points to the available sample\n",
    "#model file\n",
    "W2V_MODEL = os.path.join(curdir,'..','data','sample_model.bin')\n",
    "\n",
    "\n",
    "def generate_vectors(features, model):\n",
    "    vecs = []\n",
    "    mapping = []\n",
    "    count = 0\n",
    "    for line in features:\n",
    "        words = line.split()\n",
    "        vec = []\n",
    "        flag = 0\n",
    "        for word in words:\n",
    "            word = str(word)\n",
    "            word = re.escape(word)\n",
    "            word = re.sub(r'\\\\', '', word)\n",
    "            if word in model:\n",
    "                if len(vec) > 1:\n",
    "                    vec = vec+model[word]\n",
    "                else:\n",
    "                    vec = model[word]\n",
    "            else:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            if len(vec) > 0:\n",
    "                vecs.append(vec)\n",
    "                mapping.append(count)\n",
    "        count += 1\n",
    "    return [vecs, mapping]\n",
    "\n",
    "def cluster_try(vecs):\n",
    "    clusterVec = {}\n",
    "    clusterIdx = {}\n",
    "    no_of_clusters = 1\n",
    "    clusterIdx[0] = [0]\n",
    "    clusterVec[0] = vecs[0]\n",
    "    max_sim = 0.8\n",
    "    index = 0\n",
    "    for i in range(1, len(vecs)):\n",
    "        flag = 0\n",
    "        for j in range(no_of_clusters):\n",
    "            sim = np.dot(vecs[i], clusterVec[j])/\\\n",
    "                (np.linalg.norm(clusterVec[j]) * \\\n",
    "                 np.linalg.norm(vecs[i]))\n",
    "            if sim > max_sim:\n",
    "                flag = 1\n",
    "                max_sim = sim\n",
    "                index = j\n",
    "        if flag == 0:\n",
    "            clusterIdx[j+1] = [i]\n",
    "            clusterVec[j+1] = vecs[i]\n",
    "            no_of_clusters += 1\n",
    "        else:\n",
    "            clusterIdx[index].append(i)\n",
    "            clusterVec[index] += vecs[i]\n",
    "    return clusterIdx\n",
    "   \n",
    "model = word2vec.Word2Vec.load_word2vec_format(W2V_MODEL, binary=True)\n",
    "for doc in products_db:\n",
    "    clusters = []\n",
    "    if len(doc['features']) > 0:\n",
    "        [vecs, mapping] = generate_vectors(doc['features'], model)\n",
    "        clusters = cluster_try(vecs)\n",
    "        doc['clusters'] = clusters\n",
    "        doc.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
